{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc7cf39-accf-4790-b27b-dac10f4d0a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ----- 設定 -----\n",
    "embedding_path = \"../data/embeddings/bge_embeddings.npy\"  # 埋め込みベクトル\n",
    "metadata_path = \"../data/processed/cleaned_risk_dataset.csv\"  # メタ情報（docID, company_name など）\n",
    "output_plot_path = \"../outputs/tsne_clusters.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a1545b-d9b8-414c-b44d-f790db6717ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- データ読み込み -----\n",
    "print(\"Loading data...\")\n",
    "embeddings = np.load(embedding_path)\n",
    "df = pd.read_csv(metadata_path)\n",
    "\n",
    "# docIDが一致しているか確認（埋め込み数 = データ数）\n",
    "assert embeddings.shape[0] == len(df), \"データ数とベクトル数が一致しません\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e571dff-2c44-4510-9b23-2f107a8aa299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- クラスタリング -----\n",
    "print(\"Clustering...\")\n",
    "n_clusters = 9  # 適宜変更\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "df[\"cluster\"] = kmeans.fit_predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98754450-a3bd-4165-8dbd-335f369f308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 次元圧縮（t-SNE） -----\n",
    "print(\"Running t-SNE...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "reduced = tsne.fit_transform(embeddings)\n",
    "df[\"x\"] = reduced[:, 0]\n",
    "df[\"y\"] = reduced[:, 1]\n",
    "\n",
    "plt.title(\"t-SNE plot of Risk Embeddings (by Cluster)\")\n",
    "# 事前にoutputsディレクトリを作成しておく\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "plt.savefig(output_plot_path)\n",
    "plt.show()\n",
    "print(f\"✅ クラスタプロットを保存しました: {output_plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade447df-88b6-4b83-9538-a9327a27b8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Notebook内描画設定\n",
    "%matplotlib inline\n",
    "\n",
    "# 埋め込み読み込み\n",
    "embeddings = np.load(\"../data/embeddings/bge_embeddings.npy\")\n",
    "\n",
    "# t-SNEで次元削減\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "# KMeansクラスタリング\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# 描画\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=labels, cmap=\"tab10\", alpha=0.7)\n",
    "plt.title(\"t-SNE Visualization of Risk Embeddings\")\n",
    "plt.xlabel(\"t-SNE 1\")\n",
    "plt.ylabel(\"t-SNE 2\")\n",
    "plt.colorbar(label=\"Cluster\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97ea33b-e9ce-46b6-847c-a45d16c95048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 業種情報を追加\n",
    "import pandas as pd\n",
    "df2 = pd.read_csv('../data/processed/nikkei225_complete.csv')\n",
    "df_complete = pd.merge(df.copy(), df2[['ＥＤＩＮＥＴコード', '提出者業種']], left_on='edinet_code', right_on='ＥＤＩＮＥＴコード', how='left')\n",
    "df_complete = df_complete.rename(columns={'提出者業種': 'industry'})\n",
    "df_complete.to_csv('../data/processed/df_rag.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abdc494-7ff9-425e-83a6-68608f9e00d5",
   "metadata": {},
   "source": [
    "## RAG\n",
    "構成（llama-index + FAISS + LLM）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb27626-4d70-4654-b3e2-cd9f7f23b008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/processed/df_rag.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ea51a-e837-4a10-a560-23434d86ee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# 1. 全文を使ってTF-IDFを計算\n",
    "all_texts = df[\"token_str\"].dropna().tolist()\n",
    "vectorizer_all = TfidfVectorizer(max_features=3000)\n",
    "tfidf_all = vectorizer_all.fit_transform(all_texts)\n",
    "global_vocab = vectorizer_all.get_feature_names_out()\n",
    "\n",
    "# 2. 単語ごとのスコア平均を算出\n",
    "mean_scores_all = tfidf_all.mean(axis=0).A1\n",
    "\n",
    "# 3. 上位 N個の「共通語」を取得（除外候補）\n",
    "top_common = list(set(global_vocab[mean_scores_all.argsort()[::-1][:50]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5429f098-5ab3-470c-8848-4a4cd09b6b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "stop_words = top_common\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words=stop_words)\n",
    "\n",
    "for cluster_id in sorted(df[\"cluster\"].unique()):\n",
    "    texts = df[df[\"cluster\"] == cluster_id][\"token_str\"].dropna().tolist()\n",
    "    \n",
    "    if not texts:\n",
    "        continue\n",
    "\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    mean_scores = tfidf_matrix.mean(axis=0).A1\n",
    "    top_n = 10\n",
    "    top_idx = mean_scores.argsort()[::-1][:top_n]\n",
    "\n",
    "    print(f\"\\nクラスタ {cluster_id} の特徴語 Top {top_n}:\")\n",
    "    for i in top_idx:\n",
    "        print(f\"  {feature_names[i]}: {mean_scores[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6c13fc-9214-48e5-8cdc-3d18ebc9fbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib  # すでに入ってる場合はスキップ\n",
    "\n",
    "# 1. 全体の業種比率（期待値）\n",
    "industry_total_ratio = df_complete[\"industry\"].value_counts(normalize=True)\n",
    "\n",
    "# 2. クラスタ × 業種 の出現頻度\n",
    "cluster_industry = df_complete.groupby([\"cluster\", \"industry\"]).size().unstack(fill_value=0)\n",
    "\n",
    "# 3. 各クラスタ内の業種比率\n",
    "cluster_industry_ratio = cluster_industry.div(cluster_industry.sum(axis=1), axis=0)\n",
    "\n",
    "# 4. 補正比率（観測 / 期待）\n",
    "corrected_ratio = cluster_industry_ratio.copy()\n",
    "for col in corrected_ratio.columns:\n",
    "    corrected_ratio[col] = corrected_ratio[col] / industry_total_ratio[col]\n",
    "\n",
    "# 5. ヒートマップ表示（1.0が平均、濃いほど過剰）\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.heatmap(corrected_ratio, annot=True, cmap=\"RdBu_r\", center=1.0, fmt=\".2f\")\n",
    "plt.title(\"クラスタごとの業種構成比（全体比に対する相対値）\")\n",
    "plt.xlabel(\"業種\")\n",
    "plt.ylabel(\"クラスタ\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70fde64-916b-458a-842c-620ad5f34a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb770a56-659e-4bfc-86e6-32cfa7b449c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ✅ 1. ライブラリ読み込み\n",
    "# =========================\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import faiss\n",
    "\n",
    "from llama_index.core import Document, Settings, get_response_synthesizer, VectorStoreIndex\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank # 追加\n",
    "\n",
    "from langchain_community.llms.llamacpp import LlamaCpp\n",
    "\n",
    "# =========================\n",
    "# ✅ 2. LLMとEmbeddingの設定\n",
    "# =========================\n",
    "print(\"🔧 Setting LLM and embedder...\")\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"../models/Llama-3-ELYZA-JP-8B-q4_k_m.gguf\",\n",
    "    temperature=0.0,\n",
    "    n_ctx=2048,\n",
    "    max_tokens=512,\n",
    "    verbose=False\n",
    ")\n",
    "Settings.llm = llm\n",
    "\n",
    "# 埋め込みモデル\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"pkshatech/RoSEtta-base\", trust_remote_code=True)\n",
    "\n",
    "# =========================\n",
    "# ✅ 3. データ読み込みと準備\n",
    "# =========================\n",
    "# Assuming 'df' is loaded and prepared before this section\n",
    "# For demonstration purposes, let's create a dummy DataFrame if df is not defined\n",
    "\n",
    "assert \"token_str\" in df.columns and \"cluster\" in df.columns, \"必須カラムが不足しています\"\n",
    "\n",
    "documents = [Document(text=row[\"token_str\"]) for _, row in df.iterrows()]\n",
    "\n",
    "# =========================\n",
    "# ✅ 4. FAISS Indexの作成\n",
    "# =========================\n",
    "print(\"📦 Creating FAISS index...\")\n",
    "dimension = 1024 # Ensure this matches your embedding model's dimension (RoSEtta-base has 1024)\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "faiss_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    vector_store=faiss_store,\n",
    "    transformations=[SentenceSplitter(chunk_size=512, chunk_overlap=20)],\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# オプション：保存\n",
    "with open(\"faiss_index.pkl\", \"wb\") as f:\n",
    "    pickle.dump(index, f)\n",
    "print(\"✅ faiss_index.pkl saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0710006-4240-4a13-9ca0-96a36f1edc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ✅ 5. Query Engine セットアップ (Reranker組み込み)\n",
    "# =========================\n",
    "# Retrieverの設定\n",
    "retriever = VectorIndexRetriever(index=index, similarity_top_k=10) # 最初に多めに取得\n",
    "\n",
    "# Rerankerの設定\n",
    "reranker = SentenceTransformerRerank(\n",
    "    model=\"hotchpotch/japanese-reranker-cross-encoder-base-v1\",\n",
    "    top_n=3 # リランキング後に最終的に取得するドキュメント数\n",
    ")\n",
    "\n",
    "# Response Synthesizerの設定\n",
    "synthesizer = get_response_synthesizer(response_mode=\"tree_summarize\")\n",
    "\n",
    "# Query Engineの構築\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=synthesizer,\n",
    "    node_postprocessors=[reranker] # ここでrerankerを追加\n",
    ")\n",
    "print(\"✅ Query Engine with Reranker setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412547d6-be00-4681-8ae5-27246a8f0bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ✅ 6. クラスタごとの要約処理\n",
    "# =========================\n",
    "print(\"🧠 クラスタ要約開始...\")\n",
    "cluster_ids = sorted(df[\"cluster\"].unique())\n",
    "cluster_summaries = {}\n",
    "\n",
    "for cluster_id in cluster_ids:\n",
    "    print(f\"\\n【クラスタ {cluster_id}】要約中...\")\n",
    "\n",
    "    # 各クラスタに属するドキュメントのみをフィルタリングしてquery_engineに渡す\n",
    "    # ただし、現状のコードではquery_engineはFAISSインデックス全体に対してクエリを実行します。\n",
    "    # 特定のクラスタのドキュメントのみを要約したい場合は、\n",
    "    # そのクラスタのドキュメントを一時的に別のインデックスに入れてクエリするか、\n",
    "    # 既存のdfからフィルタリングしたデータを使ってSummaryIndexを作成するなどの工夫が必要です。\n",
    "    # ここでは、簡略化のため、全体のインデックスに対してクエリを発行し、プロンプトで制御することを想定します。\n",
    "\n",
    "    # クエリにクラスタ固有の情報を追加し、LLMにそのクラスタのドキュメントから要約させる\n",
    "    # 実際には、cluster_idに対応するドキュメントを抽出して渡す必要がありますが、\n",
    "    # 現在のLlamaIndexのRetrieverQueryEngineの設計では、\n",
    "    # クエリ時に動的に特定のクラスタのドキュメントのみを対象とすることは直接的ではありません。\n",
    "    # そのため、プロンプトで「このクラスタに関連する」というニュアンスを持たせることで、\n",
    "    # LLMがretrieverから取得したドキュメントの中から関連性の高いものを選択するよう促します。\n",
    "\n",
    "    # ここをより厳密にするには、cluster_idごとに新しいVectorStoreIndexを作成し、\n",
    "    # そのindexに対してquery_engineを構築する必要があります。\n",
    "    # 例:\n",
    "    # cluster_docs = [Document(text=row[\"token_str\"]) for _, row in df[df[\"cluster\"] == cluster_id].iterrows()]\n",
    "    # cluster_index = VectorStoreIndex.from_documents(cluster_docs, transformations=[SentenceSplitter(chunk_size=512, chunk_overlap=20)])\n",
    "    # cluster_retriever = VectorIndexRetriever(index=cluster_index, similarity_top_k=5)\n",
    "    # cluster_query_engine = RetrieverQueryEngine(retriever=cluster_retriever, response_synthesizer=synthesizer, node_postprocessors=[reranker])\n",
    "    # response = cluster_query_engine.query(query)\n",
    "\n",
    "    # 現在のコードの構造を維持しつつ、要約のプロンプトを調整\n",
    "    query = (\n",
    "        \"以下は同じクラスタに属するリスク文です。\\n\"\n",
    "        \"このクラスタ特有のリスクの種類・内容を、できるだけ具体的に2～3点、箇条書きで要約してください。\\n\"\n",
    "        \"抽象的なフレーズ（例：リスク管理、サステナビリティ）ではなく、実際に文中で言及されている内容を用いてください。\"\n",
    "    )\n",
    "\n",
    "    response = query_engine.query(query)\n",
    "    summary = str(response).strip()\n",
    "    cluster_summaries[cluster_id] = summary\n",
    "    print(f\"✅ 要約完了: {summary}\")\n",
    "\n",
    "# =========================\n",
    "# ✅ 7. JSON形式で保存\n",
    "# =========================\n",
    "output_path = \"../outputs/reports/cluster_summaries.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cluster_summaries, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"📄 クラスタ要約を {output_path} に保存しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab4686a-0f52-410e-b694-a9c368d7ef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 長すぎるクラスタを防ぐために、最大件数を制限（例: 30件）\n",
    "MAX_DOCS_PER_CLUSTER = 30\n",
    "cluster_docs_df = cluster_docs_df.head(MAX_DOCS_PER_CLUSTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c78b193-3f76-487c-89c9-fc1287e75380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ✅ 1. ライブラリ読み込み\n",
    "# =========================\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import faiss\n",
    "\n",
    "from llama_index.core import Document, Settings, get_response_synthesizer, VectorStoreIndex\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "from langchain_community.llms.llamacpp import LlamaCpp\n",
    "\n",
    "# =========================\n",
    "# ✅ 2. LLMとEmbeddingの設定\n",
    "# =========================\n",
    "print(\"🔧 Setting LLM and embedder...\")\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"../models/Llama-3-ELYZA-JP-8B-q4_k_m.gguf\",\n",
    "    temperature=0.1,\n",
    "    n_ctx=8192,  # 8192に変更\n",
    "    max_tokens=512,\n",
    "    verbose=False,\n",
    "    n_gpu_layers=32\n",
    ")\n",
    "Settings.llm = llm\n",
    "\n",
    "# 埋め込みモデル\n",
    "# HuggingFaceEmbeddingのインスタンスを直接変数に格納する\n",
    "embed_model_instance = HuggingFaceEmbedding(model_name=\"pkshatech/RoSEtta-base\", trust_remote_code=True)\n",
    "Settings.embed_model = embed_model_instance # Settingsにも設定しておく\n",
    "\n",
    "# =========================\n",
    "# ✅ 3. データ読み込みと準備\n",
    "# =========================\n",
    "df = pd.read_csv('../data/processed/df_rag.csv')\n",
    "assert \"risk_text\" in df.columns and \"cluster\" in df.columns, \"必須カラムが不足しています\"\n",
    "\n",
    "# =========================\n",
    "# ✅ 4. Rerankerの設定 (一度だけ定義)\n",
    "# =========================\n",
    "# Rerankerの設定\n",
    "reranker = SentenceTransformerRerank(\n",
    "    model=\"hotchpotch/japanese-reranker-cross-encoder-base-v1\",\n",
    "    top_n=3 # リランキング後に最終的に取得するドキュメント数\n",
    ")\n",
    "print(\"✅ Reranker setup complete.\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# ✅ 5. クラスタごとの要約処理 (メインの変更点)\n",
    "# =========================\n",
    "print(\"🧠 クラスタ要約開始...\")\n",
    "cluster_ids = sorted(df[\"cluster\"].unique())\n",
    "cluster_summaries = {}\n",
    "\n",
    "# Node Parser (SentenceSplitter) はここでも使用\n",
    "node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
    "\n",
    "for cluster_id in cluster_ids:\n",
    "    print(f\"\\n【クラスタ {cluster_id}】要約中...\")\n",
    "\n",
    "    # 1. 特定クラスタのドキュメントのみを抽出\n",
    "    cluster_docs_df = df[df[\"cluster\"] == cluster_id]\n",
    "    cluster_documents = [Document(text=row[\"risk_text\"]) for _, row in cluster_docs_df.iterrows()]\n",
    "\n",
    "    if not cluster_documents:\n",
    "        print(f\"⚠️ クラスタ {cluster_id} にドキュメントがありません。スキップします。\")\n",
    "        cluster_summaries[cluster_id] = \"このクラスタには要約するドキュメントがありません。\"\n",
    "        continue\n",
    "\n",
    "    # 長すぎるクラスタを防ぐために、最大件数を制限（例: 30件）\n",
    "    MAX_DOCS_PER_CLUSTER = 30\n",
    "    # ここでdfの件数を制限するのではなく、cluster_documentsのリストを直接制限する\n",
    "    cluster_documents = cluster_documents[:MAX_DOCS_PER_CLUSTER]\n",
    "\n",
    "\n",
    "    # 2. そのクラスタのドキュメントから一時的なFAISS Indexを作成\n",
    "    # 各クラスタでFAISS Indexを再作成するため、次元を再定義\n",
    "    dimension = 1024 # RoSEtta-baseの次元\n",
    "    cluster_faiss_index = faiss.IndexFlatL2(dimension)\n",
    "    cluster_faiss_store = FaissVectorStore(faiss_index=cluster_faiss_index)\n",
    "\n",
    "    cluster_index = VectorStoreIndex.from_documents(\n",
    "        cluster_documents,\n",
    "        vector_store=cluster_faiss_store,\n",
    "        transformations=[node_parser], # 既存のnode_parserを使用\n",
    "        embed_model=embed_model_instance, # 明示的に埋め込みモデルを指定する\n",
    "        show_progress=False # 各クラスタでprogress barを出すと煩雑になるためオフ\n",
    "    )\n",
    "    print(f\"  --- クラスタ {cluster_id} の一時インデックスを作成しました。\")\n",
    "\n",
    "    # 3. クラスタ専用のQuery Engineを構築\n",
    "    cluster_retriever = VectorIndexRetriever(index=cluster_index, similarity_top_k=5) # クラスタ内の取得数を5に変更\n",
    "    synthesizer = get_response_synthesizer(response_mode=\"tree_summarize\") # Synthesizerは再利用可能\n",
    "\n",
    "    cluster_query_engine = RetrieverQueryEngine(\n",
    "        retriever=cluster_retriever,\n",
    "        response_synthesizer=synthesizer,\n",
    "        node_postprocessors=[reranker] # ここでrerankerを追加\n",
    "    )\n",
    "    print(f\"  --- クラスタ {cluster_id} のQuery Engineを設定しました。\")\n",
    "\n",
    "    # 4. 要約クエリの実行\n",
    "    query = (\n",
    "        \"以下は同じクラスタに属するリスク文です。\\n\"\n",
    "        \"このクラスタ特有のリスクの種類・内容を、できるだけ具体的に2～3点、箇条書きで要約してください。\\n\"\n",
    "        \"抽象的なフレーズ（例：リスク管理、サステナビリティ）ではなく、実際に文中で言及されている内容を用いてください。\\n\"\n",
    "        \"出力は日本語で、200文字以内に収めてください。\"\n",
    "    )\n",
    "\n",
    "    response = cluster_query_engine.query(query)\n",
    "    summary = str(response).strip()\n",
    "    cluster_summaries[cluster_id] = summary\n",
    "    print(f\"✅ 要約完了: {summary}\")\n",
    "\n",
    "# =========================\n",
    "# ✅ 6. JSON形式で保存\n",
    "# =========================\n",
    "output_path = \"../outputs/reports/cluster_summaries.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cluster_summaries, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"📄 クラスタ要約を {output_path} に保存しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fd04aa-daf8-47dd-bb67-c8bdb41a75e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import \n",
    "import json\n",
    "import torch\n",
    "import gc # ガベージコレクション用\n",
    "\n",
    "# LLMの接続にopenaiライブラリとLlamaIndexのOpenAILLMラッパーを使用\n",
    "from openai import OpenAI\n",
    "from llama_index.llms.openai import OpenAI as LlamaIndexOpenAI\n",
    "\n",
    "from llama_index.core import Document, Settings, get_response_synthesizer, VectorStoreIndex\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore # faiss-gpu-cu12などをインストールした場合\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "# =========================\n",
    "# 1. LLMとEmbeddingの設定\n",
    "# =========================\n",
    "print(\"🔧 LLMと埋め込みモデルを設定中...\")\n",
    "\n",
    "# LLMの設定 (llama.cpp server + OpenAI互換API経由)\n",
    "# OpenAIクライアントをllama.cppサーバーに接続します\n",
    "# api_keyはダミーで問題ありません。\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8080/v1\", # llama-serverがリッスンしているURL\n",
    "    api_key=\"dummy_api_key\"             # ダミーキーでOK\n",
    ")\n",
    "\n",
    "# LlamaIndexのOpenAILLMラッパーを設定\n",
    "# モデル名もllama-serverではダミーで問題ありません\n",
    "llm = LlamaIndexOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",              # 任意のモデル名\n",
    "    api_key=\"dummy_api_key\",            # ダミーキーでOK\n",
    "    api_base=\"http://localhost:8080/v1\",# llama-serverがリッスンしているURL\n",
    "    temperature=0.6,                    # 生成の多様性 (0.0-1.0)。高いほど多様な回答。\n",
    "    max_tokens=512,                     # LLMが生成する最大トークン数。要約の長さに応じて調整。\n",
    "    # LlamaIndexのOpenAILLMは、OpenAI APIのチャットテンプレートに準拠するため、\n",
    "    # llama.cppサーバーが期待するChatML形式と互換性があります。\n",
    "    # system_promptやquery_wrapper_promptはLlamaIndexが適切に処理します。\n",
    ")\n",
    "Settings.llm = llm\n",
    "\n",
    "# 埋め込みモデルの設定 (これはこれまでと同じ)\n",
    "# pkshatech/RoSEtta-base は初回実行時に自動でダウンロードされます\n",
    "embed_model_instance = HuggingFaceEmbedding(model_name=\"pkshatech/RoSEtta-base\", trust_remote_code=True)\n",
    "Settings.embed_model = embed_model_instance\n",
    "\n",
    "print(\"✅ LLMと埋め込みモデルの設定が完了しました。\")\n",
    "\n",
    "# =========================\n",
    "# 2. データ読み込みと準備\n",
    "# =========================\n",
    "# dfは前もって定義されていることを前提とします。\n",
    "# 例: df = pd.read_csv(\"your_data.csv\")\n",
    "# あるいは、テスト用にダミーデータを作成（本番ではこれを削除）\n",
    "df = pd.read_csv('../data/processed/df_rag.csv')\n",
    "\n",
    "assert \"risk_text\" in df.columns and \"cluster\" in df.columns, \"DataFrameに必須カラム 'risk_text' または 'cluster' が見つかりません。\"\n",
    "\n",
    "# =========================\n",
    "# 3. Rerankerの設定\n",
    "# =========================\n",
    "# Rerankerの設定 (これはこれまでと同じ)\n",
    "# hotchpotch/japanese-reranker-cross-encoder-base-v1 も自動ダウンロードされます\n",
    "reranker = SentenceTransformerRerank(\n",
    "    model=\"hotchpotch/japanese-reranker-cross-encoder-base-v1\",\n",
    "    top_n=3 # リランキング後に最終的に取得するドキュメント数\n",
    ")\n",
    "print(\"✅ Rerankerの設定が完了しました。\")\n",
    "\n",
    "# =========================\n",
    "# 4. クラスタごとの要約処理\n",
    "# =========================\n",
    "print(\"🧠 クラスタごとの要約を開始します...\")\n",
    "# cluster_idsをPythonのint型に変換して取得 (TypeError対策)\n",
    "cluster_ids = sorted([int(c_id) for c_id in df[\"cluster\"].unique()])\n",
    "cluster_summaries = {}\n",
    "\n",
    "# Node Parser (SentenceSplitter)\n",
    "node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
    "\n",
    "for cluster_id in cluster_ids:\n",
    "    print(f\"\\n--- 【クラスタ {cluster_id}】要約中...\")\n",
    "\n",
    "    # 1. 特定クラスタのドキュメントのみを抽出\n",
    "    cluster_docs_df = df[df[\"cluster\"] == cluster_id]\n",
    "    cluster_documents = [Document(text=row[\"risk_text\"]) for _, row in cluster_docs_df.iterrows()]\n",
    "\n",
    "    if not cluster_documents:\n",
    "        print(f\"⚠️ クラスタ {cluster_id} に要約するドキュメントがありません。スキップします。\")\n",
    "        cluster_summaries[int(cluster_id)] = \"このクラスタには要約するドキュメントがありません。\"\n",
    "        continue\n",
    "\n",
    "    # 長すぎるクラスタを防ぐために、最大件数を制限（例: 30件）\n",
    "    MAX_DOCS_PER_CLUSTER = 30\n",
    "    cluster_documents = cluster_documents[:MAX_DOCS_PER_CLUSTER]\n",
    "\n",
    "    # 2. そのクラスタのドキュメントから一時的なFAISS Indexを作成\n",
    "    # RoSEtta-baseの埋め込み次元は1024\n",
    "    dimension = 1024\n",
    "    cluster_faiss_index = faiss.IndexFlatL2(dimension)\n",
    "    cluster_faiss_store = FaissVectorStore(faiss_index=cluster_faiss_index)\n",
    "\n",
    "    cluster_index = VectorStoreIndex.from_documents(\n",
    "        cluster_documents,\n",
    "        vector_store=cluster_faiss_store,\n",
    "        transformations=[node_parser],\n",
    "        embed_model=embed_model_instance, # 明示的に埋め込みモデルを指定\n",
    "        show_progress=False\n",
    "    )\n",
    "    print(f\"  --- クラスタ {cluster_id} の一時インデックスを作成しました。\")\n",
    "\n",
    "    # 3. クラスタ専用のQuery Engineを構築\n",
    "    cluster_retriever = VectorIndexRetriever(index=cluster_index, similarity_top_k=5)\n",
    "    synthesizer = get_response_synthesizer(response_mode=\"tree_summarize\")\n",
    "\n",
    "    cluster_query_engine = RetrieverQueryEngine(\n",
    "        retriever=cluster_retriever,\n",
    "        response_synthesizer=synthesizer,\n",
    "        node_postprocessors=[reranker]\n",
    "    )\n",
    "    print(f\"  --- クラスタ {cluster_id} のQuery Engineを設定しました。\")\n",
    "\n",
    "    # 4. 要約クエリの実行\n",
    "    # ここでのクエリはLLMに直接渡されます\n",
    "    query = (\n",
    "        \"このクラスタに属するリスク文を2～3点、箇条書きで具体的に要約してください。\\n\"\n",
    "        \"抽象的なフレーズは避け、文中で言及されている内容のみを用いてください。\\n\"\n",
    "        \"出力は日本語で、200文字以内に収めてください。\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = cluster_query_engine.query(query)\n",
    "        summary = str(response).strip()\n",
    "        cluster_summaries[int(cluster_id)] = summary # キーが確実にPythonのint型であることを保証\n",
    "        print(f\"✅ 要約完了: {summary}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ クラスタ {cluster_id} の要約中にエラーが発生しました: {e}\")\n",
    "        cluster_summaries[int(cluster_id)] = f\"要約中にエラーが発生しました: {e}\"\n",
    "\n",
    "# =========================\n",
    "# 5. JSON形式で保存\n",
    "# =========================\n",
    "output_path = \"../outputs/reports/cluster_summaries.json\"\n",
    "# outputs/reports ディレクトリが存在しない場合に作成\n",
    "import os\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cluster_summaries, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"📄 クラスタ要約結果を {output_path} に保存しました。\")\n",
    "\n",
    "# =========================\n",
    "# 6. メモリ解放の試み\n",
    "# =========================\n",
    "# llama.cppサーバーのモデルは別プロセスでメモリ管理されるため、Pythonスクリプト側での解放は不要です。\n",
    "# ただし、埋め込みモデルのインスタンスやLlamaIndexのSettingsは削除しておくと良いでしょう。\n",
    "if 'embed_model_instance' in locals():\n",
    "    del embed_model_instance\n",
    "# if 'Settings' in locals() and hasattr(Settings, 'llm'): # この行と次行を削除またはコメントアウト\n",
    "#     del Settings.llm                                     # この行を削除またはコメントアウト\n",
    "# if 'Settings' in locals() and hasattr(Settings, 'embed_model'): # この行と次行を削除またはコメントアウト\n",
    "#     del Settings.embed_model                             # この行を削除またはコメントアウト\n",
    "\n",
    "# Pythonのガベージコレクタを強制的に実行\n",
    "gc.collect()\n",
    "\n",
    "# PyTorchのCUDAキャッシュをクリア（念のため）\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"GPUメモリの解放を試みました（該当する場合）。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93956694-41c7-451f-af3a-b2492f2099f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5080edeb-b3c8-44b5-8e25-af7776578123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a83af80-cb10-46c9-a207-51776bbda68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ../models/Llama-3-ELYZA-JP-8B-q4_k_m.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Llama-3-8B-optimal-merged-stage2\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: control token: 128255 '<|reserved_special_token_250|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128254 '<|reserved_special_token_249|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128253 '<|reserved_special_token_248|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128251 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128246 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128243 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128240 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128239 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128238 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128237 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128232 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128228 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128227 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128225 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128222 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128215 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128211 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128210 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128204 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128203 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128201 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128197 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128196 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128195 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128193 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128191 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128190 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128185 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128184 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128182 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128181 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128177 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128176 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128175 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128174 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128173 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128172 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128168 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128167 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128166 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128165 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128162 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128159 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128155 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128153 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128152 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128151 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128148 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128146 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128144 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128143 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128141 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128139 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128138 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128135 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128133 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128132 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128131 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128130 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128128 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128125 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128121 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128120 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128119 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128116 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128112 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128109 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128107 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128106 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128105 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128103 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128100 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128099 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128098 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128094 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128088 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128087 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128086 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128084 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128082 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128078 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128075 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128073 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128072 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128070 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128065 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128064 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128062 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128060 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128059 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128057 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128056 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128054 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128051 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128043 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128042 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128041 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128040 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128035 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128033 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128032 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128029 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128025 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128024 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128021 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128020 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128019 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128018 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128015 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128013 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128012 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128010 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128005 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128004 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128249 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128187 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128180 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128134 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128179 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128037 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128045 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128089 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128212 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128104 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128205 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128142 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128028 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128126 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128198 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128071 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128092 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128183 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128140 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128226 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128052 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128053 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128058 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128150 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128149 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128209 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128169 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128157 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128038 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128178 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128091 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128115 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128233 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128145 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128039 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128136 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128170 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128236 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128154 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128049 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128023 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128016 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128113 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128158 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128223 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128156 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128008 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128085 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128160 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128110 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128247 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128122 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128050 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128221 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128244 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128248 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128213 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128208 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128074 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128234 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128083 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128224 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128055 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128097 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128206 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128081 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128068 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128067 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128046 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128194 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128069 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128220 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128214 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128108 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128200 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128048 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128027 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128114 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128235 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128252 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128199 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128129 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128245 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128164 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128124 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128102 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128036 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128229 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128163 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128127 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128111 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128231 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128188 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128061 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128137 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128093 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128095 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128189 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128090 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128147 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128219 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128230 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128217 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128031 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128030 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128250 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128192 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128096 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128186 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128207 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128171 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128080 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128077 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128101 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128079 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128216 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128014 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128047 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128202 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128044 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128161 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128017 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128066 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128242 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128118 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128076 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128034 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128241 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128026 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128218 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128063 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128117 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128011 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128022 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128123 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
      "llm_load_print_meta: general.name     = Llama-3-8B-optimal-merged-stage2\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  4685.30 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 8192\n",
      "llama_new_context_with_model: n_ctx_per_seq = 8192\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 500000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   560.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '8192', 'general.name': 'Llama-3-8B-optimal-merged-stage2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.file_type': '15', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: llama-3\n",
      "You try to use a model that was created with version 3.0.1, however, your version is 2.7.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 【2018年】クラスタごとの要約を開始します...\n",
      "--- 【2018年】埋め込みベクトルの生成中...\n",
      "--- 【2018年】最適なクラスタ数を見つけるためエルボー法を実行中 (K=2～15)...\n",
      "--- 【2018年】最適なクラスタ数: 8\n",
      "--- 【2018年】エルボープロットを '../outputs/elbow_plots/elbow_method_2018.png' に保存しました。\n",
      "--- 【2018年】KMeansクラスタリング実行中 (K=8)...\n",
      "--- 【クラスタ 0】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4607 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    51 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  100708.05 ms /  4658 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 3646 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3646 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   323 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  108453.70 ms /  3969 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 0\n",
      "2018年、当社グループにまとめられている主要な事業リスクは以下のとおりです。\n",
      "\n",
      "1. 消費増税や五輪後の不況発生に係るリスク\n",
      "2. 少子高齢化長寿命化に係るリスク\n",
      "3. 所得の二極化に係るリスク\n",
      "4. テクノロジーの進化に係るリスク\n",
      "5. ＣＳＲの重要性アップに係るリスク\n",
      "6. 減損に係るリスク\n",
      "7. 資金調達に係るリスク\n",
      "8. 資産運用に関するリスク\n",
      "9. 国内外の自然災害による多額の保険金支払のリスク\n",
      "10. 流動性リスク\n",
      "11. 再保険取引に関するリスク\n",
      "12. 経済環境社会環境等の予期せぬ変化により損失が発生するリスク\n",
      "13. 保険業界の競争に関するリスク\n",
      "14. 国内生命保険事業に関するリスク\n",
      "15. 海外事業に関するリスク\n",
      "16. 関連事業に関するリスク\n",
      "17. 情報漏洩等に関するリスク\n",
      "18. システムリスク\n",
      "19. 人事労務に関するリスク\n",
      "20. 事業運営に関するリスク\n",
      "21. 事業中断に関するリスク\n",
      "22. 統合のシナジーが十分に発揮されないリスク\n",
      "--- 【クラスタ 1】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4263 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4263 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   118 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  103997.74 ms /  4381 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 2412 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  2412 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   120 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   60311.99 ms /  2532 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 1\n",
      "2018年は、分譲マンションの新規供給量や販売状況の変動が業績に影響を与える可能性があります。建設資材労務等の急激な高騰や調達難が生じた場合、生産能力の低下が生じて業績に悪影響を及ぼす可能性があります。分譲マンション建設用地の供給や取引先デベロッパーの事業規模の変動が業績に影響を与える可能性があります。\n",
      "--- 【クラスタ 2】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4410 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4410 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   130 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  108927.78 ms /  4540 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 612 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   612 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   378 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   41814.07 ms /   990 tokens\n",
      "Llama.generate: 165 prefix-match hit, remaining 1602 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1602 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /  1023 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  120225.19 ms /  2625 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 2\n",
      "2018年における主要な事業リスクは、以下の通りです。\n",
      "\n",
      "- コンシューマープロダクツ事業の消費者ニーズの変化への対応ができない場合、ブランド価値を落とす可能性がある。\n",
      "- 流通の変化に対応した販売活動や新たな提案ができない場合、当社グループの財政状態及び経営成績に影響を及ぼす可能性がある。\n",
      "- 原材料の調達で予想を超えて市場価格に急激な変動が生じた場合、当社グループの財政状態及び経営成績に影響を及ぼす可能性がある。\n",
      "- 生産工場の爆発火災事故、原材料購入先のトラブル、電力や水などの社会インフラの機能不全、有害物質による環境汚染、感染症の蔓延、テロ、政変、暴動などが発生し、商品の市場への供給に支障をきたした場合、当社グループへの信用、財政状態及び経営成績に重大な影響を及ぼす可能性がある。\n",
      "- 為替の変動による影響を受けるが、為替予約取引や通貨スワップ取引などにより為替変動リスクをヘッジすることにしており、経営成績に与える影響を軽減している。\n",
      "- 繰延税金資産や減損処理の影響で、有形固定資産、無形資産、のれん、繰延税金資産等の資産について、期待されるキャッシュフローが生み出せない場合、当社グループの財政状態及び経営成績に影響を及ぼす可能性がある。\n",
      "- 人財の確保ができず、多様で優秀な人財が不足した場合、当社グループの財政状態及び経営成績に影響を及ぼす可能性がある。\n",
      "- 法規制の遵守ができず、重大な法令違反を起こした場合、当社グループへの信用、財政状態及び経営成績に影響を及ぼす可能性がある。\n",
      "- 情報管理が不適切で、顧客情報が漏洩した場合、顧客個人に支払う損害賠償による費用の発生や当社グループの社会的信用の失墜による売上高の減少が考えられ、当社グループの業績や財務状況に悪影響を及ぼす可能性がある。\n",
      "- 自然災害や政治情勢の変化などが発生し、当社グループの店舗や製造工場、物流センター等の設備や人的資源に被害が発生した場合、営業活動に支障が生じ、業績や財務状況に悪影響を及ぼす可能性がある。\n",
      "\n",
      "Original Answer: \n",
      "- コンシューマープロダクツ事業の消費者ニーズの変化への対応ができない場合、ブランド価値を落とす可能性がある。\n",
      "- 流通の変化に対応した販売活動や新たな提案ができない場合、当社グループの財政状態及び経営成績に影響を及ぼす可能性がある。\n",
      "- 原材料の調達で予想を超えて市場価格に急激な変動が生じた場合、当社グループの財政状態及び経営成績に影響を及ぼす可能性がある。\n",
      "- 生産工場の爆発火災事故、原材料購入先のトラブル、電力や水などの社会インフラの機能不全、有害物質による環境汚染、感染症の蔓延、テロ、政変、暴動などが発生し、商品の市場への供給に支障をきたした場合、当社グループへの信用、財政状態及び経営成績に重大な影響を及ぼす可能性がある。\n",
      "- 為替の変動による影響を受けるが、為替予約取引や通貨スワップ取引などにより為替変動リスクをヘッジすることにしており、経営成績に与える影響を軽減している。\n",
      "- 繰延税金資産や減損処理の影響で、有形固定資産、無形資産、のれん、繰延税金資産等の資産について、期待されるキャッシュフローが生み出せない場合、当社グループの財政状態及び経営成績に影響を及ぼす可能性がある。\n",
      "- 人財の\n",
      "--- 【クラスタ 3】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4391 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4391 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   139 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  109402.88 ms /  4530 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 791 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   791 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   230 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   33302.54 ms /  1021 tokens\n",
      "Llama.generate: 82 prefix-match hit, remaining 1352 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1352 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   320 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   53032.29 ms /  1672 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 3\n",
      " - 災害等の発生による人的被害や建物設備の損傷、営業制約、消費マインドの冷え込み等による業績や財務状況の影響\n",
      "         - 感染症の流行による施設利用客の減少、鉄道の列車運行等の事業運営の支障、業績や財務状況の影響\n",
      "         - 事故等の発生による人的被害や事業の中断、被害者に対する損害賠償責任や施設の復旧等に伴う費用の発生、顧客の信頼及び社会的評価の低下による業績や財務状況の影響\n",
      "         - 保有資産及び商品の瑕疵欠陥による改善原状復帰補償等にかかる費用の発生や信用低下等による業績や財務状況の影響\n",
      "         - システム障害の発生による事業活動の支障、業績や財務状況の影響\n",
      "         - コンプライアンス違反や個人情報管理の不備、情報開示の不適切さ等による社会的信頼の低下や法的責任の発生、業績や財務状況の影響\n",
      "         - 人材の確保や法的規制の変更、金利の変動、重要な訴訟等による事業の継続や業績の影響\n",
      "--- 【クラスタ 4】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4355 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4355 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  103806.43 ms /  4455 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 2708 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  2708 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   225 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   76541.12 ms /  2933 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 4\n",
      " - 経営人材リスク\n",
      "         - 競合リスク\n",
      "         - 海外事業リスク\n",
      "         - 為替リスク\n",
      "         - 製造物責任リスク\n",
      "         - 営業秘密個人情報漏洩リスク\n",
      "         - 天候リスク\n",
      "         - 災害リスク\n",
      "         - 紛争訴訟リスク\n",
      "         - 経済環境消費動向の変化のリスク\n",
      "         - 輸出リスク\n",
      "         - 為替レートの変動リスク\n",
      "         - 新興国市場のリスク\n",
      "         - 原材料の価格変動サプライヤーの供給能力に係るリスク\n",
      "         - 生産活動に使用される原材料の供給停止や遅延のリスク\n",
      "         - 外部委託先や社内工程における製造の遅延や不良のリスク\n",
      "         - 電力不足や電力費の上昇が生産活動及び販売活動に及ぼすリスク\n",
      "--- 【クラスタ 5】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4297 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4297 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    55 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   98268.98 ms /  4352 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 3726 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3726 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   430 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  121381.13 ms /  4156 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 5\n",
      "1. 景気の動向の影響を受けるリスク\n",
      "         - 競合他社の存在や技術革新の影響を受けるリスク\n",
      "         - リクルートブランドの毀損や評判の悪化のリスク\n",
      "         - サービス提供媒体の変化に伴うリスク\n",
      "         - 技術革新によるリスク\n",
      "         - クライアントの需要動向の変化に関するリスク\n",
      "         - ユーザーの需要動向の変化に関するリスク\n",
      "         - 主要なソフトウェアベンダ又はハードウェアベンダの製品にウイルス対策やサイバーセキュリティ機能が付加される可能性についてのリスク\n",
      "         - 当社グループは連結売上のほとんどを単一の事業領域に依存していることにより当該市場の需要低下の影響を大きく受けてしまう可能性についてのリスク\n",
      "         - 技術革新により当社グループの各種製品及びサービスが陳腐化してしまう可能性についてのリスク\n",
      "         - ハードウェア製品の製造リスク在庫リスクについてのリスク\n",
      "         - 他社との戦略的提携から期待通りの成果があげられない可能性についてのリスク\n",
      "         - 当社グループの競合先企業が日本市場で成功を収めた場合に当社グループの日本市場での売上高やマーケットシェアが低下する可能性についてのリスク\n",
      "         - 将来の企業買収により利益の減少やオペレーションコストの増加が発生する可能性についてのリスク\n",
      "         - ハッカーやクラッカーによる当社グループのシステムへの不正侵入により当社グループの信用が失墜する可能性についてのリスク\n",
      "         - 当社グループ関係者による情報漏洩リスク\n",
      "--- 【クラスタ 6】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4379 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4379 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    60 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  101912.54 ms /  4439 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 561 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   561 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    88 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   17947.17 ms /   649 tokens\n",
      "Llama.generate: 95 prefix-match hit, remaining 1924 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1924 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   141 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   51295.97 ms /  2065 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 6\n",
      " - 安全対策の不備による事故の発生\n",
      "         - 自然災害やテロ等の外部要因による鉄道インフラの被害\n",
      "         - コンピュータシステムの機能障害や個人情報流出による業務運営の影響\n",
      "         - 有利子負債の返済や金利の変動による財務の影響\n",
      "         - コンプライアンス違反や法令の変更による社会的信用の失墜や行政処分のリスク\n",
      "         - 国際事業の政治的、社会的、経済的リスクや大型プロジェクトの投資リスク\n",
      "--- 【クラスタ 7】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4275 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4275 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   110 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  103880.02 ms /  4385 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 2961 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  2961 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   240 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   83643.41 ms /  3201 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 7\n",
      "2018年、当社グループの主要な事業リスクは以下の通りです。\n",
      "\n",
      "- 製品の安全性や品質に問題が発生した場合、患者や製品の安定供給に影響を及ぼす可能性があります。\n",
      "- 副作用が発現した場合、販売の停止や回収等の措置が必要となり、業績に重要な影響を及ぼす可能性があります。\n",
      "- 新薬開発の不確実性やジェネリック医薬品の低価格での販売により、売上収益が減少する可能性があります。\n",
      "- 工場の閉鎖または操業停止のリスク、環境に関するリスク、災害等に関するリスクがあります。\n",
      "- 法規制に関するリスク、個人消費動向に関するリスク、食の安全性に関するリスク、原材料価格の高騰等に関するリスクがあります。\n",
      "- 特許権の保護期間満了や特許権の侵害、訴訟に関するリスクがあります。\n",
      "\n",
      "These risks can impact our company's industry, financial performance, and reputation.\n",
      "\n",
      "--- 【2018年】の要約結果を '../outputs/reports/cluster_summaries_2018.json' に保存しました。\n",
      "\n",
      "🧠 【2019年】クラスタごとの要約を開始します...\n",
      "--- 【2019年】埋め込みベクトルの生成中...\n",
      "--- 【2019年】最適なクラスタ数を見つけるためエルボー法を実行中 (K=2～15)...\n",
      "--- 【2019年】最適なクラスタ数: 4\n",
      "--- 【2019年】エルボープロットを '../outputs/elbow_plots/elbow_method_2019.png' に保存しました。\n",
      "--- 【2019年】KMeansクラスタリング実行中 (K=4)...\n",
      "--- 【クラスタ 0】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4439 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4439 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  105679.24 ms /  4539 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 2270 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  2270 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   204 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   64493.96 ms /  2474 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 0\n",
      " - 法的規制の変更や運賃改定の結果、当社の業績に影響を及ぼす可能性がある。\n",
      "         - 少子高齢化や人口減少が進行することで、当社の業績や財務状況に影響を及ぼす可能性がある。\n",
      "         - 長期債務や借入金の返済が重くのしかかり、当社の財務状況に影響を及ぼす可能性がある。\n",
      "         - 自然災害やテロ等の外部要因により、当社の鉄道事業に大きな影響が生じる可能性がある。\n",
      "         - コンピュータシステムの障害やサイバー攻撃等により、当社の業務運営に影響を与える可能性がある。\n",
      "         - 超電導リニアによる中央新幹線計画の進捗や実現性が、当社の経営や財務に影響を及ぼす可能性がある。\n",
      "--- 【クラスタ 1】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4357 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4357 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  100573.96 ms /  4421 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 618 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   618 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   128 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   21484.30 ms /   746 tokens\n",
      "Llama.generate: 100 prefix-match hit, remaining 2480 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  2480 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   250 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   73517.12 ms /  2730 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 1\n",
      "2019年、当社グループの主要な事業リスクは、技術変化の読みと対応の遅れによる成長性や業績の悪影響、主要市場の経済状況の変化による業績の悪化、競合の激化による価格競争の激化と業績の悪化、部品原材料の価格変動と為替レートの変動による業績の悪化、技術強化領域の設定や資源の投下の遅れによる成長性や業績の悪影響、人材の確保や育成の遅れによる成長性や業績の悪影響、イノベーションや技術革新の遅れによる成長性や業績の悪影響、などです。\n",
      "\n",
      "Original Answer: 主要な2019年の事業リスクは、主要市場の経済状況の変化による業績の悪化、競合の激化による価格競争の激化と業績の悪化、部品原材料の価格変動と為替レートの変動による業績の悪化です。\n",
      "--- 【クラスタ 2】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4370 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4370 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    40 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   97837.43 ms /  4410 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 3080 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3080 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   183 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   82214.93 ms /  3263 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 2\n",
      " - 資産運用に関するリスク\n",
      "         - 自然災害による多額の保険金支払のリスク\n",
      "         - 再保険取引に関するリスク\n",
      "         - シェアリングエコノミーの進展に係るリスク\n",
      "         - テクノロジーの進化に係るリスク\n",
      "         - 人事労務に関するリスク\n",
      "         - 事業運営に関するリスク\n",
      "         - 事業中断に関するリスク\n",
      "         - 統合のシナジーが十分に発揮されないリスク\n",
      "         - 情報漏洩等に関するリスク\n",
      "         - 海外事業に関するリスク\n",
      "         - 関連事業に関するリスク\n",
      "         - システムリスク\n",
      "         - ESG環境社会ガバナンスの重要性向上に係るリスク\n",
      "--- 【クラスタ 3】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4318 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4318 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   139 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  107533.81 ms /  4457 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 345 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   345 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   112 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   14814.35 ms /   457 tokens\n",
      "Llama.generate: 34 prefix-match hit, remaining 3307 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3307 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   228 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   90990.52 ms /  3535 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 3\n",
      "2019年の主要な事業リスクは、研究開発の途中で有効性や安全性が承認に必要な水準を充たさないことが判明した場合の研究開発中断やコスト回収のリスク、特許権が満了した場合の後発品の市場参入による売上低下のリスク、新たな副作用の確認による使用上の注意の記載や使用方法の制限、販売中止や回収等のリスクです。\n",
      "\n",
      "Original Answer: 2019年の主要な事業リスクは、研究開発の途中で有効性や安全性が承認に必要な水準を充たさないことが判明した場合の研究開発中断やコスト回収のリスク、特許権が満了した場合の後発品の市場参入による売上低下のリスク、新たな副作用の確認による使用上の注意の記載や使用方法の制限、販売中止や回収等のリスクです。\n",
      "\n",
      "--- 【2019年】の要約結果を '../outputs/reports/cluster_summaries_2019.json' に保存しました。\n",
      "\n",
      "🧠 【2020年】クラスタごとの要約を開始します...\n",
      "--- 【2020年】埋め込みベクトルの生成中...\n",
      "--- 【2020年】最適なクラスタ数を見つけるためエルボー法を実行中 (K=2～15)...\n",
      "--- 【2020年】最適なクラスタ数: 8\n",
      "--- 【2020年】エルボープロットを '../outputs/elbow_plots/elbow_method_2020.png' に保存しました。\n",
      "--- 【2020年】KMeansクラスタリング実行中 (K=8)...\n",
      "--- 【クラスタ 0】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    98 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  106430.77 ms /  4552 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 3643 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3643 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   261 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  102804.67 ms /  3904 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 0\n",
      "1. 法的規制の変更や運賃改定の結果、業績に影響を及ぼす可能性がある。\n",
      "2. 少子高齢化や人口減少が進行し、鉄道事業の需要が減少する可能性がある。\n",
      "3. 自然災害や感染症の発生等の外的要因により、安全確保が難しい状況に陥り、業績に影響を及ぼす可能性がある。\n",
      "4. 調達金利の変動や資源価格の高騰が、業績に影響を及ぼす可能性がある。\n",
      "5. 情報セキュリティ対策や個人情報の管理が不十分な場合、業績に影響を及ぼす可能性がある。\n",
      "\n",
      "Original Answer:\n",
      "- 法的規制の変更や運賃改定の結果、業績に影響を及ぼす可能性がある。\n",
      "- 少子高齢化や人口減少が進行し、鉄道事業の需要が減少する可能性がある。\n",
      "- 自然災害や感染症の発生等の外的要因により、安全確保が難しい状況に陥り、業績に影響を及ぼす可能性がある。\n",
      "--- 【クラスタ 1】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4430 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4430 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   118 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  107243.66 ms /  4548 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 2606 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  2606 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   309 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   81415.70 ms /  2915 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 1\n",
      "2020年は、米中貿易摩擦の影響を受ける可能性が高く、中国向け売上の比率が高いため、政治状況の悪化が進むと当社グループの業績に悪影響を与える可能性があります。また、為替変動や金利変動の影響を受ける可能性があり、半導体FPD市場の動向や特定顧客の設備投資動向の影響も受ける可能性があります。さらに、借入金に係る契約の財務制限条項に抵触し借入先金融機関の請求があった場合、期限の利益を喪失する可能性があり、当社グループの社債やその他の借入金についても連動して期限の利益を喪失する可能性があります。加えて、パンデミックに関するリスクや情報セキュリティに関するリスク、製品の品質と安全に関するリスク、環境負荷低減に関するリスク、係争法令違反に関するリスク、固定資産の減損に関するリスク、為替の変動に関するリスク、事故災害に関するリスク、重要なサプライヤーに関するリスク、自然災害に関するリスク、戦争やテロに関するリスク、訴訟や法的手続きに関するリスクなど、多くのリスクが存在します。\n",
      "--- 【クラスタ 2】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4178 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4178 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   141 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  103394.71 ms /  4319 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 3803 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3803 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   316 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  112232.91 ms /  4119 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 2\n",
      "2020年6月22日現在、当社グループが認識している主要な事業リスクは以下の通りです。\n",
      "\n",
      "1. 新型コロナウイルス感染拡大による与信費用の増加、保有有価証券の評価損益悪化、外貨建資金調達の不安定化、原油価格下落等を起因とした金融市場混乱の拡大。\n",
      "2. 競争環境の変化、社会構造、産業構造の変化によるビジネス戦略の影響。\n",
      "3. 人財に関するリスク、人財が確保できない場合や人財の一斉流出等が発生した場合の業務運営や業績財務状況の悪影響。\n",
      "\n",
      "Original Answer: \n",
      "- 新型コロナウイルス感染拡大による与信費用の増加、保有有価証券の評価損益悪化、外貨建資金調達の不安定化、原油価格下落等を起因とした金融市場混乱の拡大\n",
      "         - 競争環境の変化、社会構造、産業構造の変化によるビジネス戦略の影響\n",
      "         - 人財に関するリスク、人財が確保できない場合や人財の一斉流出等が発生した場合の業務運営や業績財務状況の悪影響\n",
      "--- 【クラスタ 3】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4333 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4333 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   127 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  106197.81 ms /  4460 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 2306 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  2306 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   331 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   75939.60 ms /  2637 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 3\n",
      "2020年の主要な事業リスクは、以下の通りです。\n",
      "\n",
      "- 信用リスク：景気悪化や融資先の経営状況悪化に伴う不良債権の増加、破綻先の整理回収活動の困難化。\n",
      "- 市場関連リスク：株価下落や金利上昇に伴う有価証券の減損や評価損の発生。\n",
      "- 流動性リスク：調達環境の悪化や資金の確保に高い金利での資金調達を余儀なくされることによる資金繰りの悪化。\n",
      "- 新型コロナウイルス感染症拡大に関するリスク：不特定多数の顧客や取引先の感染拡大に伴う事業の停止や遅延、経済活動の縮小による業績の悪化。\n",
      "\n",
      "Original Answer: \n",
      "- 信用リスク：景気悪化や融資先の経営状況悪化に伴う不良債権の増加、破綻先の整理回収活動の困難化。\n",
      "- 市場関連リスク：株価下落や金利上昇に伴う有価証券の減損や評価損の発生。\n",
      "- 流動性リスク：調達環境の悪化や資金の確保に高い金利での資金調達を余儀なくされることによる資金繰りの悪化。\n",
      "--- 【クラスタ 4】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4411 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4411 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   138 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  109597.56 ms /  4549 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 2002 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  2002 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   440 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   78441.80 ms /  2442 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 4\n",
      " - 主要なソフトウェアベンダやハードウェアベンダがウイルス対策やサイバーセキュリティ機能を付加した製品を低価格で販売することで、当社グループの競争力が低下する可能性。\n",
      "         - 技術革新のスピードが速く、当社グループの製品やサービスが陳腐化する可能性。\n",
      "         - ハードウェア製品の製造を委託している製造業者が当社グループの注文通りに製品を生産できず、生産体制を築けないリスク。\n",
      "         - 自然災害や有事（紛争、クーデター、テロ等）が発生し、当社グループの事業が停止や制限を受ける可能性。\n",
      "         - 新型コロナウイルス感染症の流行拡大が続き、当社グループの業績に影響を与える可能性。\n",
      "         - 将来の企業買収により利益の減少やオペレーションコストの増加が発生する可能性。\n",
      "         - ハッカー等による当社グループのシステムへの不正侵入により、当社グループの信用が失墜する可能性。\n",
      "         - 当社グループ関係者による情報漏洩リスク。\n",
      "\n",
      "Original Answer: \n",
      "- 主要なソフトウェアベンダやハードウェアベンダがウイルス対策やサイバーセキュリティ機能を付加した製品を低価格で販売することで、当社グループの競争力が低下する可能性。\n",
      "         - 技術革新のスピードが速く、当社グループの製品やサービスが陳腐化する可能性。\n",
      "         - ハードウェア製品の製造を委託している製造業者が当社グループの注文通りに製品を生産できず、生産体制を築けないリスク。\n",
      "--- 【クラスタ 5】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4533 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4533 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   113 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  111413.62 ms /  4646 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 3500 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3500 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   246 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   97640.76 ms /  3746 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 5\n",
      "2020年の主要な事業リスクは、以下の通りです。\n",
      "\n",
      "1. 経済環境の変化に伴う景気後退や保護主義政策の流れが当社の経営成績に悪影響を及ぼす可能性がある。\n",
      "2. 為替レートの変動が当社の業績に影響を与えるため、為替予約やヘッジを実施している。\n",
      "3. オフィスでの出力機会の減少やワークスタイルの変化が当社のオフィス事業に悪影響を及ぼす可能性がある。\n",
      "\n",
      "Original Answer: \n",
      "- 経済環境の変化に伴う景気後退や保護主義政策の流れが当社の経営成績に悪影響を及ぼす可能性がある。\n",
      "         - 為替レートの変動が当社の業績に影響を与えるため、為替予約やヘッジを実施している。\n",
      "         - オフィスでの出力機会の減少やワークスタイルの変化が当社のオフィス事業に悪影響を及ぼす可能性がある。\n",
      "--- 【クラスタ 6】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4248 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4248 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    94 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  101430.19 ms /  4342 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 3416 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3416 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   206 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   91878.76 ms /  3622 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 6\n",
      "2020年の主要な事業リスクは、以下の通りです。\n",
      "\n",
      "1. 人口動態や市場嗜好の変化に伴う競争環境の激化や法令改正の影響によるリスク\n",
      "2. 新薬の研究開発の長期化や期待通りの有効性が認められない場合のリスク\n",
      "3. 知的財産権の侵害や特許権の満了に伴う製品の売上収益の減少のリスク\n",
      "\n",
      "Original Answer:\n",
      "- 人口動態や市場嗜好の変化に伴う競争環境の激化や法令改正の影響によるリスク\n",
      "- 新薬の研究開発の長期化や期待通りの有効性が認められない場合のリスク\n",
      "- 知的財産権の侵害や特許権の満了に伴う製品の売上収益の減少のリスク\n",
      "--- 【クラスタ 7】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4478 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4478 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   108 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  108624.93 ms /  4586 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 3076 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3076 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   354 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   97028.75 ms /  3430 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 7\n",
      "2020年、ソニーの主要な事業リスクは以下の通りです。\n",
      "\n",
      "1. 新型コロナウイルス感染拡大がソニーの事業活動、業績、財政状態に悪影響を及ぼす可能性がある。\n",
      "2. 激化する競争に直面し、収益や営業利益率の低下のリスクがある。\n",
      "3. コンスーマーエレクトロニクス事業では、価格競争、製品サイクルの短期化、販売流通チャネルの変化に直面する。\n",
      "\n",
      "仮にソニーが技術その他の競争力を持つ分野においてその優位性を保てなくなった場合、ソニーのコンスーマー製品に対して頻繁に影響を及ぼす継続的な価格下落又はその事業に影響を及ぼすコスト圧力について効果的に予測し対応できない場合、既存の事業モデルや消費者の嗜好が変化した場合、又はソニーのコンスーマー製品の平均価格の下落スピードが当該製品の製造原価削減のスピードを上回った場合には、ソニーの業績及び財政状態に悪影響を及ぼす可能性があります。\n",
      "\n",
      "加えて、ソニーは競争力を維持し消費者の需要を喚起し製品及びサービスの革新を実現するために研究開発投資を行う必要があり、また新しい製品及びサービスの頻繁な導入を適切に管理しなければなりません。\n",
      "\n",
      "--- 【2020年】の要約結果を '../outputs/reports/cluster_summaries_2020.json' に保存しました。\n",
      "\n",
      "🧠 【2021年】クラスタごとの要約を開始します...\n",
      "--- 【2021年】埋め込みベクトルの生成中...\n",
      "--- 【2021年】最適なクラスタ数を見つけるためエルボー法を実行中 (K=2～15)...\n",
      "--- 【2021年】最適なクラスタ数: 8\n",
      "--- 【2021年】エルボープロットを '../outputs/elbow_plots/elbow_method_2021.png' に保存しました。\n",
      "--- 【2021年】KMeansクラスタリング実行中 (K=8)...\n",
      "--- 【クラスタ 0】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4410 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4410 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   115 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  106361.61 ms /  4525 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 2932 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  2932 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   636 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  118917.34 ms /  3568 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 0\n",
      "2021年の事業リスクとして、以下の点が挙げられます。\n",
      "\n",
      "- 経済状況の悪化による雇用環境や家計の悪化が当社の業績に悪影響を及ぼす可能性がある。\n",
      "- 競争環境の激化や新規参入により、当社の競争力が低下し、業績に悪影響を及ぼす可能性がある。\n",
      "- 大規模災害や新型コロナウイルス感染症の流行が当社の事業活動や業績に悪影響を及ぼす可能性がある。\n",
      "- 資金調達のリスク: 当社グループの主な資金調達方法は銀行など金融機関からの借入金や社債、コマーシャルペーパー（CP）の発行など資本市場からの調達であり、短期借入金やCPなど調達期間が一年以内のものが相当額あり、また一年以内に返済償還予定の長期負債もあることから、当社グループ固有の要素である業績悪化や信用格付の格下げなどや外部の要素である経済金融危機や自然災害など、さまざまな要因によって流動性リスクが増加すると事業活動や業績に重大な影響を及ぼす可能性がある。\n",
      "- マーケットリスク: 当社グループは上場会社、非上場会社の株式、ベンチャー企業、投資ファンド、債券、不動産、不動産ファンドなどへの投資を行っており、投資資産の価格が市場において下落した場合には当社グループの業績及び財政状態に悪影響を及ぼす可能性がある。\n",
      "- 金融商品の減損、貸倒引当金: 当社グループは各事業においてさまざまな融資を行っており、多数の顧客に対する債権を保有しており、国内外の経済環境、景気後退に伴う雇用環境、家計、個人消費等の状況の変化により多くの顧客において契約条件に従った債権の返済がなされず、当社グループの業績及び財政状態に悪影響を及ぼす可能性がある。\n",
      "\n",
      "Original Answer: \n",
      "- 経済状況の悪化による雇用環境や家計の悪化が当社の業績に悪影響を及ぼす可能性がある。\n",
      "- 競争環境の激化や新規参入により、当社の競争力が低下し、業績に悪影響を及ぼす可能性がある。\n",
      "- 大規模災害や新型コロナウイルス感染症の流行が当社の事業活動や業績に悪影響を及ぼす可能性がある。\n",
      "--- 【クラスタ 1】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 3997 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3997 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    35 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   89342.80 ms /  4032 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 863 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   863 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    97 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   24508.71 ms /   960 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 3134 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3134 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   190 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   83854.72 ms /  3324 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 1\n",
      " - プロジェクトの受注及び遂行に関するリスク\n",
      "         - カントリーリスク\n",
      "         - 自然災害疫病等に関するリスク\n",
      "         - 新型コロナウイルス感染症の世界的な感染拡大に伴うプロジェクトの遅延や中断、顧客の最終投資判断の先送りによる新規プロジェクトの計画変更や延期の可能性\n",
      "         - 為替変動リスク\n",
      "         - 工事従事者の不足賃金高騰リスク\n",
      "         - 資機材原燃材料費の高騰リスク\n",
      "         - 環境保全リスク\n",
      "         - 品質管理リスク\n",
      "         - 情報セキュリティリスク\n",
      "         - 労働力の不足リスク\n",
      "         - 法的規制リスク\n",
      "--- 【クラスタ 2】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4325 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4325 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    90 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  102271.30 ms /  4415 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 3503 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3503 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   199 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   93284.65 ms /  3702 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 2\n",
      "2021年の主要な事業リスクは以下の通りです。\n",
      "\n",
      "1. 新型コロナウイルス感染症の影響で、受注や生産に支障が生じる可能性があります。\n",
      "2. 情報セキュリティ管理態勢の高度化やプロジェクトリスクに対するマネジメントの徹底が必要です。\n",
      "3. 品質に関するリスクとして、システムの安定稼働やデータセンターの安全確保が重要です。\n",
      "\n",
      "Original Answer:\n",
      "- 新型コロナウイルス感染症の影響で、受注や生産に支障が生じる可能性がある。\n",
      "- 情報セキュリティ管理態勢の高度化やプロジェクトリスクに対するマネジメントの徹底が必要。\n",
      "- 品質に関するリスクとして、システムの安定稼働やデータセンターの安全確保が重要。\n",
      "--- 【クラスタ 3】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4408 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4408 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    72 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  103461.12 ms /  4480 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 3595 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3595 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   439 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  117806.38 ms /  4034 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 3\n",
      "1. 単一の事業領域に依存していることによる影響とリスクの可能性\n",
      "2. 技術革新や業界の変化により当社グループの各種製品及びサービスが陳腐化してしまう可能性\n",
      "3. ハードウエア製品の製造リスク在庫リスクについて\n",
      "4. 情報セキュリティマネジメントシステムの国際規格ISO27001及びJISQ15001プライバシーマークを取得し、業務委託先または従業員との間で機密保持目的の契約の締結、情報管理規定の整備、社員及び委託先への教育や周知徹底、インフラのセキュリティ強化、社内情報システムへの外部からの侵入防止対策を講じる等管理の強化徹底と漏洩の防止に努めるが、上記リスクを防げない可能性があります。\n",
      "5. 中間販売業者が当社グループの各種製品及びサービスを返品する可能性や、競合先企業の製品販売に注力する可能性があります。\n",
      "6. 当社グループの事業領域は拡大をしており、成長を支えるマネジメントや従業員等の人的リソースは限られるため、人材の獲得確保並びに従業員に対する教育研修、オペレーションシステム会計システム等の情報システムの整備、経営及び管理体制の有効活用が必要です。\n",
      "7. 当社グループが属するサイバーセキュリティ業界は市場競争が激化しており、優秀な人材の確保は競合各社とも技術革新を支える重要な課題となっており、人材の流出についても対策が必要です。\n",
      "\n",
      "Please note that the refined answer is based on the provided context and may not be exhaustive.\n",
      "--- 【クラスタ 4】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4259 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4259 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   158 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  107909.67 ms /  4417 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 3623 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3623 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   175 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   94831.73 ms /  3798 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 4\n",
      "2021年の主要な事業リスクは、以下の3つです。\n",
      "\n",
      "1. クレジットクオリティの悪化による与信先の業績悪化や担保価値の下落による与信関連費用の増加\n",
      "2. 保有有価証券の価値下落による保有有価証券の価値の下落や市場流動性の低下\n",
      "3. 新型コロナウイルス感染症拡大や金融緩和による過剰流動性、グローバルな地政学リスクの顕在化、インフレ金利上昇懸念の台頭などを背景に金融市場が不安定化することによる流動性リスクや外貨調達の不安定化\n",
      "--- 【クラスタ 5】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4420 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4420 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   109 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  106298.57 ms /  4529 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 643 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   643 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   181 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   25874.44 ms /   824 tokens\n",
      "Llama.generate: 145 prefix-match hit, remaining 3357 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3357 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   785 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  146415.69 ms /  4142 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 5\n",
      "2021年の主要な事業リスクは以下の通りです。\n",
      "\n",
      "1. 新型コロナウイルス感染症の感染再拡大や世界経済の影響による業績悪化の可能性\n",
      "2. 環境課題や気候変動に対する社会的要請の高まりによる企業ブランド価値の低下や代替素材の推進の可能性\n",
      "3. 原燃料の調達リスクや原油価格の変動による採算の悪化や固定資産の減損損失の可能性\n",
      "4. 為替相場の変動や金利の変動による資金調達や有価証券の価値の変動の可能性\n",
      "5. 将来予測等の前提条件の変動に伴う退職給付債務や繰延税金資産の計上の可能性\n",
      "6. グローバル事業展開に関わるリスク（米中対立、制裁関税、為替変動、原材料の調達リスク、製造物責任、法規制コンプライアンス、知的財産、訴訟、為替の変動、海外事業展開）\n",
      "7. 製造物責任に関わるリスク（品質の欠陥に起因する大規模な製品回収、PL保険でカバーできない損害賠償等の損失の発生、顧客からの信頼や社会的信用の失墜）\n",
      "8. 事故災害に関わるリスク（爆発火災有害物質の漏洩などの事故災害の未然防止及び災害発生時の被害の極小化、原燃料設備メンテナンス部品やサービスの提供などを担っているサプライヤーにおける事故災害の発生）\n",
      "9. 法規制コンプライアンスに関わるリスク（独占禁止法遵守、コンプライアンスの徹底、法令違反を起こした場合の顧客からの信頼や社会的信用の失墜、損害賠償責任や罰金が課されることなど）\n",
      "10. 訴訟に関わるリスク（独占禁止法遵守指針の定期的見直し、競合他社との接触に関するガイドラインの制定、競合他社との取引会合の事前審査、役員従業員向けセミナーの開催、遵守状況に関する社内聴取入札情報の管理及び入札部署を対象とした法務部監査等の様々な施策）\n",
      "\n",
      "Original Answer: \n",
      "- 新型コロナウイルス感染症の感染再拡大や世界経済の影響による業績悪化の可能性\n",
      "- 環境課題や気候変動に対する社会的要請の高まりによる企業ブランド価値の低下や代替素材の推進の可能性\n",
      "- 原燃料の調達リスクや原油価格の変動による採算の悪化や固定資産の減損損失の可能性\n",
      "- 為替相場の変動や金利の変動による資金調達や有価証券の価値の変動の可能性\n",
      "- 将来予測等の前提条件の変動に伴う退職給付債務や繰延税金資産の計上の可能性\n",
      "--- 【クラスタ 6】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4415 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4415 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    88 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  104531.17 ms /  4503 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 2857 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  2857 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   182 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   76130.89 ms /  3039 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 6\n",
      " - 経済環境や金融市場の動向の影響を受ける。\n",
      "         - 為替相場や金利の変動が、外貨建ての営業債権や負債に影響を与える。\n",
      "         - 市況変動や新型コロナウイルス感染症の流行が、ICT市場や顧客の事業に悪影響を与える。\n",
      "         - 海外での事業展開に伴うカントリーリスクや外国為替相場の変動が、当社の事業に影響を与える。\n",
      "         - 製品の欠陥や製造物責任が、当社の事業に悪影響を与える。\n",
      "\n",
      "Note: The refined answer includes the new context information, specifically the risks related to COVID-19, country risks, and foreign exchange risks.\n",
      "--- 【クラスタ 7】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4429 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4429 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   109 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  107468.31 ms /  4538 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 2213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  2213 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   118 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   55833.12 ms /  2331 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 7\n",
      "2021年の主要な事業リスクは、法的規制の変更や運賃改定の結果、業績に影響を及ぼす可能性があること、出生率の低下や少子高齢化の急激な加速が、沿線地域で起きた場合、業績に影響を及ぼす可能性があること、パンデミック等の発生が、運輸事業やレジャー事業の利用者減少を招き、業績に影響を及ぼす可能性があることです。\n",
      "\n",
      "--- 【2021年】の要約結果を '../outputs/reports/cluster_summaries_2021.json' に保存しました。\n",
      "\n",
      "🧠 【2022年】クラスタごとの要約を開始します...\n",
      "--- 【2022年】埋め込みベクトルの生成中...\n",
      "--- 【2022年】最適なクラスタ数を見つけるためエルボー法を実行中 (K=2～15)...\n",
      "--- 【2022年】最適なクラスタ数: 11\n",
      "--- 【2022年】エルボープロットを '../outputs/elbow_plots/elbow_method_2022.png' に保存しました。\n",
      "--- 【2022年】KMeansクラスタリング実行中 (K=11)...\n",
      "--- 【クラスタ 0】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    25 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   98610.16 ms /  4487 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 2643 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  2643 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    93 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   63534.77 ms /  2736 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 0\n",
      " - 戦略リスク\n",
      "         - 気候変動リスク\n",
      "         - 相場変動リスク\n",
      "         - 資金調達コスト増加リスク\n",
      "         - 資産減損等リスク\n",
      "         - 大規模災害リスク\n",
      "         - 経営人材に関わるリスク\n",
      "         - カントリーリスク\n",
      "         - 環境に関わるリスク\n",
      "--- 【クラスタ 1】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4294 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4294 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    66 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   98369.23 ms /  4360 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 3993 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3993 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   165 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  102435.58 ms /  4158 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 1\n",
      "2022年6月22日現在、当社グループが認識している主要な事業リスクは以下の通りです。\n",
      "\n",
      "1. 市場の混乱による保有有価証券の価値下落\n",
      "2. クレジットクオリティの悪化による与信関連費用の増加\n",
      "3. 流動性リスクによる外貨資金繰りの悪化や調達コストの上昇\n",
      "\n",
      "Original Answer:\n",
      "- 市場の混乱による保有有価証券の価値下落\n",
      "         - クレジットクオリティの悪化による与信関連費用の増加\n",
      "         - 流動性リスクによる外貨資金繰りの悪化や調達コストの上昇\n",
      "--- 【クラスタ 2】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4521 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4521 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    71 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  106492.10 ms /  4592 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 3764 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3764 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   647 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  144640.92 ms /  4411 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 2\n",
      "2022年の事業リスクは以下の通りです。\n",
      "\n",
      "1. 単一の事業領域に依存していることによる影響とリスクの可能性。\n",
      "2. 技術革新や業界の変化により当社グループの各種製品及びサービスが陳腐化してしまう可能性。\n",
      "3. ハードウエア製品の製造リスク在庫リスク。\n",
      "4. 情報セキュリティガバナンスのリスク、具体的にはセキュリティインシデントの発生、情報漏洩、不正アクセス等のリスク。\n",
      "5. 中間販売業者に依存していることによるリスク、具体的には競合先企業の製品販売に注力する可能性や中間販売業者が当社グループの製品を返品する可能性等。\n",
      "6. 経営管理体制の対応の遅れや不備によるリスク、具体的には新たな人材の獲得や教育研修の遅れ、組織体制の不備等。\n",
      "7. 人材の流出や技術者並びに人材の流出によるリスク、具体的には主要な技術者や人材が流出する可能性や当社グループの技術や戦略等の重要な情報が流出する可能性等。\n",
      "8. 為替並びに金融市場の変動によるリスク、具体的には為替相場の変動や金融市場の変動による当社グループの連結決算の数値の変動や評価損の計上等。\n",
      "9. 法令違反や法令等の改正によるリスク、具体的には法令等の遵守の不備や法令等の改正による規制や制限の強化等。\n",
      "10. 主要な経営陣の離脱や法令違反によるリスク、具体的には主要な経営陣が離脱する可能性や法令違反が発生する可能性等。\n",
      "11. ユーザのキャンセルや返品等によるリスク、具体的には企業ユーザの業績見通しの悪化や経済状況の悪化等による当社グループの各種製品やサービスの購入のキャンセルや時期の延期等。\n",
      "12. 知的財産権に関するリスク、具体的には当社グループの技術や戦略等の重要な情報の流出や類似した技術の開発等。\n",
      "13. 電力不足地震等の自然災害や地政学的リスク、感染症ウイルス等によるリスク、具体的には電力不足や地震等の自然災害や感染症ウイルス等による当社グループの設備施設等の被害や当社グループの事業の停止等。\n",
      "\n",
      "These risks are not exhaustive, and the actual risks faced by the company may be different.\n",
      "--- 【クラスタ 3】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4497 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4497 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    46 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  102075.39 ms /  4543 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 565 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   565 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   151 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   22568.02 ms /   716 tokens\n",
      "Llama.generate: 82 prefix-match hit, remaining 2327 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  2327 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   193 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   64672.48 ms /  2520 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 3\n",
      " - 新型コロナウイルス感染拡大に係るリスク\n",
      "         - ウクライナ情勢に係るリスク\n",
      "         - 気候変動に係るリスク\n",
      "         - 顧客の生産計画に係るリスク（特に、国内外のメーカーからの受注生産が大部分を占めるため、顧客の生産計画の影響を直接受ける）\n",
      "         - 品質製造物責任に係るリスク\n",
      "         - 事故自然災害に係るリスク\n",
      "         - 環境に係るリスク\n",
      "         - 海外事業展開に係るリスク\n",
      "         - 財務に係るリスク\n",
      "         - 固定資産の減損に係るリスク\n",
      "         - 訴訟等に係るリスク\n",
      "--- 【クラスタ 4】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4358 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4358 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   111 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  105951.00 ms /  4469 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 2388 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  2388 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   220 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   68279.56 ms /  2608 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 4\n",
      " - 事業戦略の失敗や新規事業の収益化の遅れにより、成長が遅滞するリスク\n",
      "         - ユーザー数や利用頻度の低下、市場変化や景気変動による広告料の低下のリスク\n",
      "         - パートナーシップの構築や維持が困難になるリスク、パートナーの売上収益やトラフィックが期待値に満たないリスク\n",
      "         - 経済情勢、規制環境、市場環境の変化に伴う競争激化や新規参入者による影響のリスク\n",
      "         - 技術やビジネスモデルの移り変わりが早い情報産業において、当社グループが新たな技術やビジネスモデルに適時かつ適切に対応できず、市場変化に適した優れたサービスや技術を創出や導入ができないリスク\n",
      "--- 【クラスタ 5】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4504 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4504 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    58 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  104084.53 ms /  4562 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 1197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   141 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   35507.11 ms /  1338 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 5\n",
      " - 景気変動リスク\n",
      "         - 新型コロナウイルス感染症等の感染症拡大リスク\n",
      "         - 多額の借入金と金利変動リスク、財務制限条項のリスク\n",
      "         - 主要取引先の設備投資の規模や構造等の動向に伴うリスク\n",
      "         - 安全品質に関するリスク\n",
      "         - 個人情報の流出に関するリスク\n",
      "         - 業績の季節変動に伴うリスク\n",
      "         - 保有資産に関するリスク\n",
      "         - 取引先の信用リスク\n",
      "--- 【クラスタ 6】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4353 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4353 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    41 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   99036.45 ms /  4394 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 553 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   553 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    87 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   17318.00 ms /   640 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 3505 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3505 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   220 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   96032.25 ms /  3725 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 6\n",
      "1. 事業環境の変化に伴うリスク\n",
      "         - 原材料の市況変動に伴うリスク\n",
      "         - 製造物責任に伴うリスク\n",
      "         - 重大な法令違反に伴うリスク\n",
      "         - 知的財産権の侵害や係争に伴うリスク\n",
      "         - 訴訟に伴うリスク\n",
      "         - 為替の変動に伴うリスク\n",
      "         - 海外事業展開に伴うリスク\n",
      "         - 情報セキュリティに伴うリスク\n",
      "\n",
      "Original Answer: \n",
      "- 事業環境の変化に伴うリスク\n",
      "         - 原材料の市況変動に伴うリスク\n",
      "         - 製造物責任に伴うリスク\n",
      "         - 重大な法令違反に伴うリスク\n",
      "         - 知的財産権の侵害や係争に伴うリスク\n",
      "         - 訴訟に伴うリスク\n",
      "--- 【クラスタ 7】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4145 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4145 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   152 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  103322.86 ms /  4297 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 3710 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3710 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   703 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  146983.75 ms /  4413 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 7\n",
      "2022年において、当社グループの主要な事業リスクは以下の通りです。\n",
      "\n",
      "1. 経済環境に関するリスク: 世界的な経済活動の本格的な再開に伴い、半導体を中心に電子部品が不足するなどグローバルサプライチェーン上の問題が発生し、物価上昇圧力が一段と強まっています。米国経済は雇用所得環境の改善により回復基調を維持していますが、経済活動回復に伴う需要増加による資源価格の高騰や供給網の混乱による需給の不均衡、労働市場の人手不足などの影響によりインフレ率の上昇が続き、米連邦準備制度理事会(FRB)はインフレ抑制に向けて金融緩和の縮小ペースを加速させています。\n",
      "\n",
      "2. 新型コロナウイルス感染症に関するリスク: 新型コロナウイルス感染症は国地域によって差はあるものの変異株が出現するなど依然として感染拡大の懸念を残しており、米中の覇権争いに起因するハイテク冷戦の影響なども引き続き懸念されます。\n",
      "\n",
      "3. 情報セキュリティに関するリスク: インターネットがインフラとして定着し、情報漏洩のリスクが高まっています。特に情報サービス産業は顧客の機密情報を扱う機会が多く、より高度な情報セキュリティ管理や社員教育の徹底が求められます。\n",
      "\n",
      "4. 品質に関するリスク: 当社グループが開発する情報システムは顧客の業務の重要な基盤となることが多く、完成後の安定稼働が重要です。特に金融サービス業のシステムについては当社顧客のみでなく金融市場全体の信頼性に関わる場合もあり、その重要性を強く認識しています。\n",
      "\n",
      "5. 地震台風などの自然災害や火災爆発などの災害が発生した場合、あるいは事件事故などの不祥事案件が発生した場合の対応を誤ると企業価値の毀損につながる可能性があります。\n",
      "\n",
      "6. 米中の覇権争いに起因するハイテク冷戦の影響: 米中の覇権争いは、半導体や電子部品の供給網に影響を与え、当社の事業に悪影響を及ぼす可能性があります。\n",
      "\n",
      "7. ウクライナ情勢の緊迫化: ウクライナ情勢の緊迫化は、世界各国の安全保障並びに経済活動に大きく影響し、ロシアが主な輸出国である天然ガスなどの資源エネルギーウクライナが主な輸出国である半導体製造用ネオンガスなどの物資の供給が滞ることにより、エネルギー価格の高騰更なる半導体不足が加速するリスクがあります。\n",
      "--- 【クラスタ 8】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4443 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4443 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    34 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  100732.42 ms /  4477 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 562 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   562 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    71 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   16253.71 ms /   633 tokens\n",
      "Llama.generate: 70 prefix-match hit, remaining 3516 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3516 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   197 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   93673.83 ms /  3713 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 8\n",
      "2022年の事業リスクとして、以下のものが挙げられます。\n",
      "\n",
      "1. 新しい製品サービスのリリースに関するリスク\n",
      "2. 競争に関するリスク\n",
      "3. 景気低迷に関するリスク\n",
      "4. 自然災害等に関するリスク\n",
      "5. 新型コロナウイルス感染症の影響\n",
      "6. セキュリティに関するリスク\n",
      "7. 内部管理体制に関するリスク\n",
      "8. 人的資源に関するリスク\n",
      "9. 気候変動に関するリスク\n",
      "\n",
      "Original Answer: \n",
      "- 新しい製品サービスのリリースに関するリスク\n",
      "         - 競争に関するリスク\n",
      "         - 景気低迷に関するリスク\n",
      "         - 自然災害等に関するリスク\n",
      "         - 新型コロナウイルス感染症の影響\n",
      "         - セキュリティに関するリスク\n",
      "--- 【クラスタ 9】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4364 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4364 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    60 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  101066.04 ms /  4424 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 3704 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3704 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    86 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   87714.75 ms /  3790 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 9\n",
      " - 医療費抑制策の影響による業績の悪化\n",
      "         - 新薬開発の不確実性による研究開発投資の無駄や中止\n",
      "         - 品質問題や法令違反による製品回収や行政処分のリスク\n",
      "\n",
      "The refined answer takes into account the provided context, which includes information about the company's business, industry, and market trends.\n",
      "--- 【クラスタ 10】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4235 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4235 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   137 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  105395.39 ms /  4372 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 2714 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  2714 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   224 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   76887.58 ms /  2938 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 10\n",
      "2022年は、当社がグローバルに事業を拡大していることから、感染症の流行やテロ行為、戦争紛争等の事態に巻き込まれるリスクがあります。具体的には、開発、製造、販売、サービス等の事業活動が中断、混乱、または延期する可能性があります。また、地震等の自然災害や設備故障、人為的ミスによる大規模な災害等が発生した場合、当社の施設に影響を与える大規模な災害等が発生した場合、当社の事業への影響が考えられます。さらに、財務会計に関するリスクとして、顧客の財政状態が悪化し売掛債権が回収困難となるリスクや、当社が保有する投資有価証券の株価変動が当社の財政状態に重要な影響を及ぼすリスクがあります。\n",
      "\n",
      "--- 【2022年】の要約結果を '../outputs/reports/cluster_summaries_2022.json' に保存しました。\n",
      "\n",
      "🧠 【2023年】クラスタごとの要約を開始します...\n",
      "--- 【2023年】埋め込みベクトルの生成中...\n",
      "--- 【2023年】最適なクラスタ数を見つけるためエルボー法を実行中 (K=2～15)...\n",
      "--- 【2023年】最適なクラスタ数: 7\n",
      "--- 【2023年】エルボープロットを '../outputs/elbow_plots/elbow_method_2023.png' に保存しました。\n",
      "--- 【2023年】KMeansクラスタリング実行中 (K=7)...\n",
      "--- 【クラスタ 0】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4449 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   101 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  107873.83 ms /  4550 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 3616 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3616 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   828 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  156372.55 ms /  4444 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 0\n",
      "2023年の事業リスクとして、以下の点が挙げられます。\n",
      "\n",
      "1. 新製品の研究開発リスク: 新薬開発の難度が高まる中、開発が計画どおりに進まず、承認発売に至らない場合や、有効性や安全性の観点から開発が遅延し、または開発を中止しなければならない事態が発生する可能性があります。\n",
      "\n",
      "2. 連結売上収益に関するリスク: ラツーダの米国での独占販売期間が2023年2月に終了したことにより、後発医薬品メーカーによるラツーダの競合品が発売され、2023年度の売上収益が大幅に減少する見込みです。\n",
      "\n",
      "3. 知的財産権に関するリスク: 当社グループが保有する知的財産権が第三者に侵害された場合や、知的財産権の有効性や帰属を巡る係争が発生した場合、競争上の優位性を十分に保持できない可能性があります。\n",
      "\n",
      "4. 医療制度改革に関するリスク: 各国の医療制度改革の方向性によっては、当社グループの経営成績等に重要な影響を及ぼす可能性があります。\n",
      "\n",
      "5. 副作用に関するリスク: 市販後に予期せぬ副作用が発生した場合、当社グループの経営成績等に重要な影響を及ぼす可能性があります。\n",
      "\n",
      "6. 品質に関するリスク: 重大な品質問題が発生した場合、製品回収、行政処分、社会的信用の毀損等により、当社グループの経営成績等に重要な影響を及ぼす可能性があります。\n",
      "\n",
      "7. 主要な事業活動の前提となる事項に関するリスク: 許可等の取消し等を命ぜられた場合、当社グループの経営成績等に重要な影響を及ぼす可能性があります。\n",
      "\n",
      "8. 技術開発と技術の変化に関するリスク: 当社グループの事業分野におけるテクノロジーの急激な変化や市場ニーズの激変等から、当社グループ製品が予想より早く陳腐化する可能性があります。\n",
      "\n",
      "9. 国際活動及び海外進出に関するリスク: 海外の政治経済情勢や法整備の影響を受け、当社グループの財政状態や経営成績等に重要な影響を及ぼす可能性があります。\n",
      "\n",
      "10. 情報管理に関するリスク: 情報が漏洩した場合、営業秘密の流出による競争力の低下や顧客の信用や社会的信用の低下を招き、当社グループの経営成績等に重要な影響を及ぼす可能性があります。\n",
      "\n",
      "11. 製品の欠陥訴訟問題に関するリスク: 将来において、当社グループ製品の製造物責任や安全性などを問うクレームが発生し、当社グループの経営成績等に重要な影響を及ぼす可能性があります。\n",
      "\n",
      "12. 知的財産の悪用侵害に関するリスク: 当社グループの知的財産が第三者に悪用された場合、競争上の優位性を失う可能性があります。\n",
      "\n",
      "13. 情報セキュリティに関するリスク: サイバー攻撃やシステム障害等により、情報が漏洩した場合、当社グループの経営成績等に重要な影響を及ぼす可能性があります。\n",
      "\n",
      "Please note that the refined answer is based on the provided context and may not be exhaustive.\n",
      "--- 【クラスタ 1】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4518 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4518 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    75 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  105485.42 ms /  4593 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 2109 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  2109 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   210 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   61440.71 ms /  2319 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 1\n",
      "2023年の主要な事業リスクは、以下の通りです。\n",
      "\n",
      "1. サイバーセキュリティに関するリスク\n",
      "2. サプライチェーンマネジメントに関するリスク\n",
      "3. 新型コロナウイルス感染症によるパンデミックやロシアウクライナ情勢等のグローバル環境の変化に伴う想定外の業績影響\n",
      "\n",
      "Additionally, other potential risks include:\n",
      "\n",
      "4. 人事関連コンプライアンス対応のリスク\n",
      "5. グループガバナンスに関するリスク\n",
      "6. オフィスプリンティング市場における環境変化のリスク\n",
      "7. 戦略的買収によるオフィスサービス事業の成長のリスク\n",
      "8. 商用印刷事業の成長リスク\n",
      "9. サーマル市場の価格競争激化のリスク\n",
      "10. のれん固定資産の減損のリスク\n",
      "--- 【クラスタ 2】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4533 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4533 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  106551.90 ms /  4606 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 3721 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3721 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   151 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   94286.95 ms /  3872 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 2\n",
      "2023年の事業リスクとして、以下のリスクが認識されています。\n",
      "\n",
      "1. 中長期経営方針の達成を阻害するリスク\n",
      "2. 新型コロナウイルス感染症の影響によるリスク\n",
      "3. 欧州オセアニア地域の景気悪化によるリスク\n",
      "\n",
      "Original Answer: 主要なリスクとして、以下のリスクが認識されています。\n",
      "\n",
      "1. 中長期経営方針の達成を阻害するリスク\n",
      "2. 新型コロナウイルス感染症の影響によるリスク\n",
      "3. 欧州オセアニア地域の景気悪化によるリスク\n",
      "--- 【クラスタ 3】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4282 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4282 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   312 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  122897.27 ms /  4594 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 3902 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3902 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /  1023 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  183473.62 ms /  4925 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 3\n",
      "2023年の主要な事業リスクとして、以下のものが挙げられます。\n",
      "\n",
      "1.紛争、テロ、内乱、暴動、地政学的問題、法規制、税務、労働環境、慣習、インフラの未整備、貿易摩擦、経済や金融環境の変動、自然災害、パンデミック、人材の採用確保の困難、ユーティリティ供給不足、原燃料の価格上昇、輸送コストの上昇、サプライチェーンの途絶、経済安全保障をめぐる国際情勢の変化などが挙げられます。\n",
      "\n",
      "2.事故災害リスクとして、製造設備の定期点検の不備や自然災害による物的人的被害や環境汚染等が生じた場合、生産への影響や社会的信頼の低下等業績に影響を与える可能性があります。\n",
      "\n",
      "3.法規制コンプライアンスリスクとして、法令の変更や規制強化が行われた場合、当社グループの活動の制限やコストの増加につながる可能性があり、万が一法規制に違反し刑事民事上の責任を問われまた行政処分を課された場合には当社グループの事業展開及び経営成績に影響を与える可能性があります。\n",
      "\n",
      "4.情報セキュリティリスクとして、当社グループが保有する企業情報及び個人情報が流出し不正使用されるなどの問題が発生した場合、競争力や社会的信頼の低下等業績に影響を与える可能性があります。\n",
      "\n",
      "5.人権リスクとして、当社グループの海外グループ会社において人権侵害に関与する事案が発生し社会的信頼の低下や取引停止などに繋がり業績に影響を与える可能性があります。\n",
      "\n",
      "6.気候変動等環境課題リスクとして、気候変動や資源エネルギーをはじめとする環境課題の包括的な解決に向けて当社グループはサーキュラーエコノミーを重要な戦略と位置付けていますが、温室効果ガスGHG排出削減や省エネルギー活動の推進など気候関連の施策に加え、炭素税の賦課や排出権取引制度に代表される温室効果ガス排出規制が導入された場合、業績に影響を与える可能性があります。\n",
      "\n",
      "7.原材料の調達リスクとして、当社グループの製品の多くは国内外の需要や製品市況原油ナフサユーティリティ等の原燃料材料の価格や調達数量為替関連法規制等によって影響を受ける可能性があります。\n",
      "\n",
      "8.製造物責任リスクとして、当社グループの製品の品質の欠陥に起因する大規模な製品回収が発生するとＰＬ保険でカバーできない損害賠償等の損失の発生顧客からの信頼や社会的信用の失墜等の可能性があります。\n",
      "\n",
      "9.訴訟リスクとして、当社グループは国内及び海外事業に関連して取引先や第三者との間で訴訟その他法的手続きが発生するリスクがあります。\n",
      "\n",
      "10.為替の変動リスクとして、当社グループが国内で生産し海外へ輸出する事業では製品の輸出価格が為替変動の影響を受けます一方海外の事業拠点で生産販売する事業では異なる通貨圏との間の調達販売価格及び外貨建て資産負債の価額が為替変動の影響を受けます。\n",
      "\n",
      "11.知的財産リスクとして、当社グループは独自技術による事業製品を数多く有していますが、知的財産権への重大な侵害や当社の権利に対する係争が発生した場合、当社グループの業績に悪影響が生じる可能性があります。\n",
      "\n",
      "12.固定資産の減損リスクとして、経営環境の著しい悪化等による収益性の低下や市場価格の下落等により保有する固定資産について減損損失が発生し当社グループの業績に悪影響が生じる可能性があります\n",
      "--- 【クラスタ 4】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4478 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4478 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  103259.73 ms /  4547 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 3664 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3664 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   367 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  113438.56 ms /  4031 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 4\n",
      "2023年における主要な事業リスクは、以下の通りです。\n",
      "\n",
      "1. クレジットクオリティの悪化によるリスク\n",
      "2. 金融市場の混乱や地政学的緊張による市場流動性の低下リスク\n",
      "3. 大規模災害やサイバー攻撃等の危機発生による業務停止や情報漏洩のリスク\n",
      "4. 当行自体の構造転換やビジネスモデルの転換の遅れによる収益力の低下リスク\n",
      "5. サステナビリティ推進の遅れによるESG評価の低下と外貨調達コストの上昇リスク\n",
      "6. 人材リソースのサステナビリティのリスク（人材の不足や流出による業務運営やビジネス戦略の実現の支障）\n",
      "7. 法令遵守や金融犯罪の不備によるリスク（不祥事件の発生や処分、インサイダー取引規制違反、顧客情報の漏洩等）\n",
      "8. 環境社会課題に係るリスク（気候変動や自然災害等による業務停止や情報漏洩のリスク）\n",
      "9. 金融犯罪や不正行為のリスク（マネーローンダリングやテロ資金供与等）\n",
      "10. 流動性リスク（資金の確保や調達の困難による資金繰りの悪化や高金利での資金調達のリスク）\n",
      "\n",
      "These risks are considered to be major risks for the 2023 business year.\n",
      "--- 【クラスタ 5】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4358 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4358 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   107 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  105435.74 ms /  4465 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 3468 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3468 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   764 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  145173.19 ms /  4232 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 5\n",
      "2023年の事業リスクとして、以下の点が挙げられます。\n",
      "\n",
      "- 世界的な疫病の蔓延拡大が航空旅客需要を大幅に減少させる可能性がある。\n",
      "- 自然災害やテロ攻撃等の災害が羽田空港や成田空港の長期間閉鎖や当社グループの情報システムやIOCの機能停止を招く可能性がある。\n",
      "- 気候変動や環境規制の強化が当社グループの業績に影響を及ぼす可能性がある。\n",
      "- 新型感染症や地震、台風等の自然災害、テロ活動等のリスクが当社グループの店舗や供給網に影響を及ぼす可能性がある。\n",
      "- 情報セキュリティに関するリスク、例えば機密情報の漏洩やサイバー攻撃によるシステム停止等が生じた場合、被害者に対する損害賠償義務やサービスの大規模な停止による損害及び対応費用の発生のほか当社グループの社会的信用の低下により当社グループの事業財務状況及び業績に影響が及ぶ可能性がある。\n",
      "- 他企業の買収ＭＡ等に関するリスク、例えば買収した企業の財務内容や契約関係等について詳細な事前調査を行い極力リスクを回避するように努めるが、買収を実施した後において偶発債務や未認識債務の発生被買収企業に対し当社グループの内部統制を適切かつ有効に適用できないことにより不正行為やコンプライアンス上の問題等が発生する可能性も考えられ、買収によって新たにのれんが発生しその償却費用が増加する可能性があり、これらの要因により期待する成果を達成できない場合当社グループの事業財務状況及び業績に影響が及ぶ可能性がある。\n",
      "- 商品の開発及び調達に関するリスク、例えば商品の品質や安全性に加えお客さまが必要とされる商品やサービスをお値打ち価格で提供することが小売業の使命であり経営の重要課題であると考えており、商品開発にあたっては厳しい基準を設けて入念な品質検査を実施する等安全と安心を守るための様々な取り組みを進め、原材料や商品の調達にあたっては国内外のベストソースからの調達スケールメリットを活用した需要集約物流の効率化等の様々な施策を通じてコストの削減と安定供給を実現するが、想定を上回る原材料価格や物流コストの上昇急激な為替の変動天候不順等の影響によりメーカー各社の価格引き上げの発生や商品調達に支障が生じた場合、低環境負荷や人権配慮等への取り組みが不十分と見なされた場合売上の低下や売上原価の上昇に加えお客さまからの信頼の失墜を招いたことによるブランドの毀損により当社グループの事業財務状況及び業績に影響が及ぶ可能性があります\n",
      "--- 【クラスタ 6】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4510 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4510 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    40 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  102829.31 ms /  4550 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 2946 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  2946 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   142 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   74927.27 ms /  3088 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 6\n",
      " - 半導体市場変動による影響\n",
      "         - 新製品の開発や投入の失敗による影響\n",
      "         - 品質問題や不具合の発生による影響\n",
      "         - 原材料の供給停止や代替品の確保の難しさによる影響\n",
      "         - 為替変動による影響\n",
      "         - 災害や感染症の流行による影響\n",
      "         - 情報セキュリティに関する影響\n",
      "         - 世界的な経済環境や政治的社会的要因の変化による影響\n",
      "\n",
      "Note: The refined answer includes the additional context provided, which includes the company's business activities, risks, and challenges.\n",
      "\n",
      "--- 【2023年】の要約結果を '../outputs/reports/cluster_summaries_2023.json' に保存しました。\n",
      "\n",
      "🧠 【2024年】クラスタごとの要約を開始します...\n",
      "--- 【2024年】埋め込みベクトルの生成中...\n",
      "--- 【2024年】最適なクラスタ数を見つけるためエルボー法を実行中 (K=2～15)...\n",
      "--- 【2024年】最適なクラスタ数: 6\n",
      "--- 【2024年】エルボープロットを '../outputs/elbow_plots/elbow_method_2024.png' に保存しました。\n",
      "--- 【2024年】KMeansクラスタリング実行中 (K=6)...\n",
      "--- 【クラスタ 0】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4518 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4518 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    48 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  101978.29 ms /  4566 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 3464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   102 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   82899.78 ms /  3566 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 0\n",
      "1. 事業ポートフォリオ戦略の失敗\n",
      "         - 新製品の開発外部の技術革新の遅れ\n",
      "         - 原油価格の変動によるシェールオイル需要の変化\n",
      "\n",
      "Original Answer: \n",
      "- 事業ポートフォリオ戦略の失敗\n",
      "         - 新製品の開発外部の技術革新の遅れ\n",
      "         - 原油価格の変動によるシェールオイル需要の変化\n",
      "--- 【クラスタ 1】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4259 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4259 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   98528.42 ms /  4328 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 3321 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3321 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   157 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   85403.45 ms /  3478 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 1\n",
      "2024年の主要な事業リスクは以下の通りです。\n",
      "\n",
      "1. 営業地盤悪化による収益力低下\n",
      "2. 不良債権問題等の信用リスク\n",
      "3. 地価下落担保権設定した不動産等について想定金額で換金等ができず不良債権処理額引当金が増加\n",
      "\n",
      "Original Answer: \n",
      "・営業地盤悪化による収益力低下\n",
      "・不良債権問題等の信用リスク\n",
      "・地価下落担保権設定した不動産等について想定金額で換金等ができず不良債権処理額引当金が増加\n",
      "--- 【クラスタ 2】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4549 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4549 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    94 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  106553.89 ms /  4643 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 3607 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3607 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   215 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   97308.94 ms /  3822 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 2\n",
      "2024年の事業リスクとして、以下の主要なリスクが挙げられます。\n",
      "\n",
      "1. 大規模自然災害の発生による保険金支払の増加や出再保険料の高騰のリスク\n",
      "2. 金融マーケットの大幅な変動による資本余力の低下や保有資産価値の下落のリスク\n",
      "3. 信用リスクの大幅な増加や投融資先企業の業績悪化やデフォルトのリスク\n",
      "\n",
      "Original Answer:\n",
      "・大規模自然災害の発生による保険金支払の増加や出再保険料の高騰のリスク\n",
      "・金融マーケットの大幅な変動による資本余力の低下や保有資産価値の下落のリスク\n",
      "・信用リスクの大幅な増加や投融資先企業の業績悪化やデフォルトのリスク\n",
      "--- 【クラスタ 3】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4344 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4344 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   136 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  108238.44 ms /  4480 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 3820 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3820 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   303 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  110833.60 ms /  4123 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 3\n",
      "2024年の事業リスクとして、以下の点が挙げられます。\n",
      "\n",
      "1. プリンターの売上変動がエプソンの経営成績に影響を及ぼす可能性がある。\n",
      "2. 他社との競合が激化し、販売価格の低下や低価格品への需要のシフト、販売数量の減少などが発生する可能性がある。\n",
      "3. エプソンの技術が競合他社の技術に追い越される、もしくは新しい技術が登場した場合、エプソンの技術的な競争優位性が損なわれ、経営成績に影響を及ぼす可能性がある。\n",
      "\n",
      "Original Answer: \n",
      "- 1. プリンターの売上変動がエプソンの経営成績に影響を及ぼす可能性がある。\n",
      "         - 2. 他社との競合が激化し、販売価格の低下や低価格品への需要のシフト、販売数量の減少などが発生する可能性がある。\n",
      "         - 3. エプソンの技術が競合他社の技術に追い越される、もしくは新しい技術が登場した場合、エプソンの技術的な競争優位性が損なわれ、経営成績に影響を及ぼす可能性がある。\n",
      "\n",
      "Please note that the refined answer is based on the provided context and may not be exhaustive.\n",
      "--- 【クラスタ 4】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4429 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4429 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    51 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  100569.38 ms /  4480 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 3095 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3095 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   184 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   82181.07 ms /  3279 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 4\n",
      " - 与信費用の増加のリスク\n",
      "         - 保有有価証券の評価損益の悪化のリスク\n",
      "         - サイバー攻撃やシステム障害等の危機発生のリスク\n",
      "         - 中長期的な視点から、景気変動や競合他社の参入、業務範囲の拡大に伴うリスク、金利の変動、気候変動等のリスクが存在する。\n",
      "         - 短期的な視点から、信用リスク、特定の取引先等への高い依存度、地域経済の動向、市場リスク、流動性リスク、退職給付債務に関するリスク等のリスクが存在する。\n",
      "\n",
      "Note: The refined answer is based on the provided context and the original answer.\n",
      "--- 【クラスタ 5】要約中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Llama.generate: 1 prefix-match hit, remaining 4083 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4083 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    52 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   92621.55 ms /  4135 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 3856 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   95697.98 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3856 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   107 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   93740.52 ms /  3963 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 要約完了: クラスタ 5\n",
      "2024年の主要な事業リスクは、景気変動や社会的変革に伴うリスク、人材の確保や育成に伴うリスク、構造改革の進捗に伴うリスク、競争環境や構造変化に起因するリスク、地政学リスク、サプライチェーンリスク、情報セキュリティリスク、クライシス危機に伴うリスクなどが挙げられます。\n",
      "\n",
      "--- 【2024年】の要約結果を '../outputs/reports/cluster_summaries_2024.json' に保存しました。\n",
      "\n",
      "--- 全期間の年ごとのクラスタリングと要約プロセスが完了しました ---\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import faiss\n",
    "from llama_index.core import Document, Settings, get_response_synthesizer, VectorStoreIndex\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.node_parser import SentenceSplitter # for chunking\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from kneed import KneeLocator\n",
    "import numpy as np # for KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- LLMと埋め込みモデルの設定 ---\n",
    "# LLM (llama-server)\n",
    "Settings.llm = LlamaCPP(\n",
    "    model_path=\"../models/Llama-3-ELYZA-JP-8B-q4_k_m.gguf\",\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=1024, # 要約に必要な最大トークン数 (必要に応じて調整)\n",
    "    context_window=8192, # Llama 3 ELYZA JP 8B のコンテキストウィンドウに合わせて調整\n",
    "    # その他、llama-serverで設定したパラメータがあればここに追加できる\n",
    "    # 例: n_gpu_layers=32 (ただしこれはサーバー側で設定済みなので不要なはず)\n",
    "    # LlamaIndex 0.10.x 以降では、これらの引数は LlamaCPP 初期化時に渡す\n",
    ")\n",
    "\n",
    "# Embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"pkshatech/RoSEtta-base\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# --- Rerankerの設定 ---\n",
    "reranker = SentenceTransformerRerank(\n",
    "    model=\"hotchpotch/japanese-reranker-cross-encoder-base-v1\",\n",
    "    top_n=3,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "# --- データ読み込み ---\n",
    "# 仮に 'data/processed/all_risk_sentences.csv' に全ての期間のリスク文があるとする\n",
    "# ここでは 'cleaned_risk_text' カラムにリスク文、'date' カラムに日付が入っていると仮定\n",
    "df_all_risks = pd.read_csv('../data/processed/df_rag.csv')\n",
    "\n",
    "# 'date' カラムから 'year' カラムを抽出\n",
    "df_all_risks['year'] = pd.to_datetime(df_all_risks['date']).dt.year\n",
    "\n",
    "# --- 年ごとのループ処理 ---\n",
    "start_year = 2018\n",
    "end_year = 2024 # 2024年のデータも対象に含める場合\n",
    "\n",
    "# outputsディレクトリがなければ作成\n",
    "import os\n",
    "os.makedirs('../outputs/reports', exist_ok=True)\n",
    "\n",
    "for current_year in range(start_year, end_year + 1):\n",
    "    print(f\"\\n🧠 【{current_year}年】クラスタごとの要約を開始します...\")\n",
    "\n",
    "    # 指定年のデータのみをフィルタリング\n",
    "    df_year = df_all_risks[df_all_risks['year'] == current_year].copy()\n",
    "\n",
    "    if df_year.empty:\n",
    "        print(f\"--- {current_year}年のリスクデータが見つかりませんでした。スキップします。\")\n",
    "        continue\n",
    "\n",
    "    # フィルタリングされたデータからDocumentオブジェクトを作成\n",
    "    documents = [Document(text=row['cleaned_risk_text'], metadata={\n",
    "                    'doc_id': row['doc_id'],\n",
    "                    'date': row['date'],\n",
    "                    'company_name': row['filer_name'],\n",
    "                    'edinet_code': row['edinet_code'],\n",
    "                    'sec_code': row['sec_code'],\n",
    "                    'industry': row['industry'],\n",
    "                 }) for index, row in df_year.iterrows()]\n",
    "\n",
    "    # --- クラスタリング処理 (年ごとに実行) ---\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.preprocessing import normalize\n",
    "\n",
    "    print(f\"--- 【{current_year}年】埋め込みベクトルの生成中...\")\n",
    "    # LlamaIndexの埋め込みモデルを使って、Documentsから直接埋め込みを生成する\n",
    "    # これは時間がかかる処理なので、もし既に生成済みの埋め込みがある場合は、それをロードする方が効率的\n",
    "    embeddings_list = [Settings.embed_model.get_text_embedding(doc.text) for doc in documents]\n",
    "    \n",
    "    # numpy 配列に変換\n",
    "    X = np.array(embeddings_list)\n",
    "    \n",
    "    # 正規化 (KMeansでコサイン類似度に近い結果を得るため)\n",
    "    X_normalized = normalize(X)\n",
    "\n",
    "    # --- エルボー法による最適なクラスタ数の決定 ---\n",
    "    # ドキュメント数が少ない場合はクラスタ数を制限する\n",
    "    # 各クラスタに最低でも2つのドキュメントがあることを想定し、Kの最大値を調整\n",
    "    max_k_for_elbow = min(len(documents) // 2, 15)\n",
    "    \n",
    "    # 最小クラスタ数を2に設定（1だとクラスタリングの意味がないため）\n",
    "    if max_k_for_elbow < 2:\n",
    "        print(f\"--- {current_year}年のドキュメント数 ({len(documents)}) が少なすぎるため、クラスタリングをスキップします。最低2つのクラスタリングには4つ以上のドキュメントが必要です。\")\n",
    "        continue\n",
    "\n",
    "    distortions = []\n",
    "    # Kの範囲を2からmax_k_for_elbowまで試す\n",
    "    K_range = range(2, max_k_for_elbow + 1)\n",
    "\n",
    "    print(f\"--- 【{current_year}年】最適なクラスタ数を見つけるためエルボー法を実行中 (K=2～{max_k_for_elbow})...\")\n",
    "    for k in K_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
    "        kmeans.fit(X_normalized)\n",
    "        distortions.append(kmeans.inertia_)\n",
    "\n",
    "    # KneeLocatorを使って最適なKを自動決定\n",
    "    # S=1.0 はエルボーの鋭さの感度。調整が必要な場合もある\n",
    "    kneedle = KneeLocator(K_range, distortions, S=1.0, curve=\"convex\", direction=\"decreasing\")\n",
    "    optimal_n_clusters = kneedle.elbow\n",
    "\n",
    "    # 最適なKが見つからなかった場合や、K=1（意味がない）になる場合などへの対策\n",
    "    if optimal_n_clusters is None or optimal_n_clusters < 2:\n",
    "        # ドキュメント数に応じてデフォルト値を設定\n",
    "        if len(documents) >= 100: # ドキュメント数が多い場合\n",
    "            optimal_n_clusters = 5\n",
    "        elif len(documents) >= 20: # 中程度の場合\n",
    "            optimal_n_clusters = 3\n",
    "        else: # 少ない場合\n",
    "            optimal_n_clusters = 2\n",
    "        print(f\"--- 【{current_year}年】自動検出で最適なクラスタ数が見つからなかったか、小さすぎました。デフォルト値 {optimal_n_clusters} を使用します。\")\n",
    "    \n",
    "    print(f\"--- 【{current_year}年】最適なクラスタ数: {optimal_n_clusters}\")\n",
    "\n",
    "    # エルボープロットの描画と保存（最適点を強調）\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(K_range, distortions, marker='o')\n",
    "    if optimal_n_clusters: # 最適なKが見つかっていればプロットに点を打つ\n",
    "        plt.vlines(optimal_n_clusters, plt.ylim()[0], plt.ylim()[1], linestyles='dashed', colors='red', label=f'Optimal K = {optimal_n_clusters}')\n",
    "        plt.legend()\n",
    "    plt.xlabel('Number of clusters (K)')\n",
    "    plt.ylabel('Distortion (Inertia)')\n",
    "    plt.title(f'Elbow Method for Optimal K ({current_year}) - Optimal K: {optimal_n_clusters}')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    elbow_plot_filename = f'../outputs/elbow_plots/elbow_method_{current_year}.png'\n",
    "    plt.savefig(elbow_plot_filename)\n",
    "    plt.close()\n",
    "    print(f\"--- 【{current_year}年】エルボープロットを '{elbow_plot_filename}' に保存しました。\")\n",
    "\n",
    "    # --- クラスタリング (最適なn_clustersで実行) ---\n",
    "    print(f\"--- 【{current_year}年】KMeansクラスタリング実行中 (K={optimal_n_clusters})...\")\n",
    "    kmeans = KMeans(n_clusters=optimal_n_clusters, random_state=42, n_init='auto')\n",
    "    kmeans_labels = kmeans.fit_predict(X_normalized)\n",
    "\n",
    "    # クラスタリング結果をDocumentオブジェクトに付与\n",
    "    for i, doc in enumerate(documents):\n",
    "        doc.metadata['cluster_label'] = int(kmeans_labels[i]) # NumPy int64をPython intに変換\n",
    "\n",
    "    # クラスタごとにドキュメントをグループ化\n",
    "    clustered_documents = defaultdict(list)\n",
    "    for doc in documents:\n",
    "        clustered_documents[doc.metadata['cluster_label']].append(doc)\n",
    "\n",
    "    # --- クラスタごとの要約ループ ---\n",
    "    all_cluster_summaries = {}\n",
    "\n",
    "    for cluster_label, cluster_docs in sorted(clustered_documents.items()):\n",
    "        print(f\"--- 【クラスタ {cluster_label}】要約中...\")\n",
    "\n",
    "        # クラスタにドキュメントがない場合はスキップ\n",
    "        if not cluster_docs:\n",
    "            print(f\"--- クラスタ {cluster_label} にドキュメントがありません。スキップします。\")\n",
    "            continue\n",
    "\n",
    "        # FAISSインデックスの次元を設定（RoSEtta-baseは1024次元）\n",
    "        dimension = 1024 \n",
    "        \n",
    "        # 毎回新しいFaissIndexインスタンスを作成し、そこへドキュメントを埋め込み・追加\n",
    "        # ここで`faiss.IndexFlatL2`を使っているが、`faiss.IndexFlatIP`（内積）の方がコサイン類似度に近い\n",
    "        # (ただし、埋め込みベクトルが正規化されている場合はL2でも良い結果になる)\n",
    "        cluster_faiss_index = faiss.IndexFlatL2(dimension) \n",
    "        cluster_faiss_store = FaissVectorStore(faiss_index=cluster_faiss_index)\n",
    "\n",
    "        cluster_index = VectorStoreIndex.from_documents(\n",
    "            cluster_docs,\n",
    "            vector_store=cluster_faiss_store,\n",
    "            transformations=[SentenceSplitter(chunk_size=4096, chunk_overlap=20)],\n",
    "            show_progress=False\n",
    "        )\n",
    "\n",
    "        # Query Engineの設定\n",
    "        retriever = VectorIndexRetriever(\n",
    "            index=cluster_index,\n",
    "            similarity_top_k=5, # 関連性の高い上位K件を取得\n",
    "        )\n",
    "        my_prompt = PromptTemplate(\n",
    "            template=\"\"\"\n",
    "        以下は、事業リスクに関する情報です。\n",
    "        {context_str}\n",
    "        \n",
    "        これらの情報から、リスク文を2～3点、箇条書きで具体的に要約したものを出力してください。\n",
    "        抽象的なフレーズを避け、文中で言及されている内容のみを用いて、要約したリスク文を箇条書きで具体的に出力してください。\n",
    "        **要約結果のみを簡潔に箇条書きで出力してください。それ以外の前書きや説明は一切不要です。**\n",
    "        \"\"\"\n",
    "        )\n",
    "        response_synthesizer = get_response_synthesizer(\n",
    "            response_mode=\"compact\", # 要約モード\n",
    "            text_qa_template=my_prompt\n",
    "        )\n",
    "        query_engine = RetrieverQueryEngine(\n",
    "            retriever=retriever,\n",
    "            response_synthesizer=response_synthesizer,\n",
    "            node_postprocessors=[reranker],\n",
    "        )\n",
    "\n",
    "        # 要約クエリの実行\n",
    "        query_text = f\"このクラスタにまとめられている主要な{current_year}年の事業リスクは何ですか？\" # 年をクエリに含める\n",
    "        response = query_engine.query(query_text)\n",
    "        \n",
    "        summary_text = str(response)\n",
    "        all_cluster_summaries[f\"cluster_{cluster_label}\"] = {\n",
    "            \"summary\": summary_text,\n",
    "            \"document_count\": len(cluster_docs),\n",
    "            \"sample_risk_sentences\": [doc.text for doc in cluster_docs[:3]] # 代表的なリスク文を3つ含める\n",
    "        }\n",
    "\n",
    "        print(f\"✅ 要約完了: クラスタ {cluster_label}\")\n",
    "        print(summary_text)\n",
    "\n",
    "    # --- 結果の保存 (年ごとのファイル名に変更) ---\n",
    "    output_filename = f'../outputs/reports/cluster_summaries_{current_year}.json'\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_cluster_summaries, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"\\n--- 【{current_year}年】の要約結果を '{output_filename}' に保存しました。\")\n",
    "\n",
    "print(\"\\n--- 全期間の年ごとのクラスタリングと要約プロセスが完了しました ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43f2437-cb16-49d6-bfb3-aa7ac9b46322",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kaitai Yuho (Python 3.11)",
   "language": "python",
   "name": "kaitai-yuho-py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
